{"id": "item_000001", "instruction": "Write a Python function that generates all permutations of a given string.", "user_input": "Generate all unique orderings of the characters in 'abc', including cases like 'cab' and 'bca'.", "category": "coding", "difficulty": "medium", "skills_tested": ["problem-solving", "algorithm design"], "expected_answer_type": "function definition", "answer": "**Generating Permutations of a String**\n=====================================\n\nHere's a Python function that generates all permutations of a given string using recursion.\n\n```python\nimport itertools\n\ndef generate_permutations(s):\n    \"\"\"\n    Generate all unique orderings of the characters in a given string.\n    \n    Args:\n        s (str): The input string.\n    \n    Returns:\n        list: A list of all unique permutations of the input string.\n    \"\"\"\n    return [''.join(p) for p in itertools.permutations(s)]\n\n# Example usage\ninput_string = 'abc'\npermutations = generate_permutations(input_string)\n\nprint(\"Permutations of '{}'\".format(input_string))\nfor i, perm in enumerate(permutations):\n    print(\"{}. {}\".format(i+1, perm))\n```\n\n**Alternative Recursive Solution**\n---------------------------------\n\nIf you prefer a recursive solution without using the `itertools` module:\n\n```python\ndef generate_permutations(s):\n    \"\"\"\n    Generate all unique orderings of the characters in a given string.\n    \n    Args:\n        s (str): The input string.\n    \n    Returns:\n        list: A list of all unique permutations of the input string.\n    \"\"\"\n    if len(s) == 1:\n        return [s]\n    else:\n        result = []\n        for i, char in enumerate(s):\n            rest = s[:i] + s[i+1:]\n            for perm in generate_permutations(rest):\n                result.append(char + perm)\n        return result\n\n# Example usage\ninput_string = 'abc'\npermutations = generate_permutations(input_string)\n\nprint(\"Permutations of '{}'\".format(input_string))\nfor i, perm in enumerate(permutations):\n    print(\"{}. {}\".format(i+1, perm))\n```\n\nBoth functions will output all unique permutations of the input string.", "judge": {"score_0_10": 9, "novel": true, "difficulty": "medium", "tags": [], "rationale": "The answer is mostly correct and complete. It follows the instruction perfectly by providing two different solutions to generate all permutations of a given string. The code is clear, well-structured, and easy to understand. However, there are no tags as the answer does not contain any errors or issues."}, "score_0_10": 9.0, "ema_global": 9.0, "ema_category": 9.0, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T04:38:02.325814+00:00"}
{"id": "item_000002", "instruction": "Design and explain an unsupervised learning approach for this problem.", "user_input": "A customer segmentation project has access to purchase history data, product catalogs, and user demographics. Develop a clustering-based solution using k-means, including feature selection, model parameters, and potential visualization techniques.", "category": "ml", "difficulty": "medium", "skills_tested": ["unsupervised learning", "clustering algorithms", "feature engineering"], "expected_answer_type": "written explanation", "answer": "**Unsupervised Learning Approach: Customer Segmentation using K-Means Clustering**\n\n### Problem Statement\nDevelop an unsupervised learning approach to segment customers based on their purchase history data, product catalogs, and user demographics.\n\n### Data Preparation\n\n1. **Data Collection**: Gather the following datasets:\n\t* Purchase History Data (e.g., transaction IDs, dates, products purchased)\n\t* Product Catalogs (e.g., product IDs, descriptions, categories)\n\t* User Demographics (e.g., age, location, income level)\n2. **Data Preprocessing**:\n\t* Handle missing values using imputation techniques (e.g., mean, median, or regression-based imputation)\n\t* Normalize/scale numerical features to ensure equal importance\n\t* Convert categorical variables into numerical representations (e.g., one-hot encoding, label encoding)\n\n### Feature Selection\n\n1. **Relevant Features**: Select features that are relevant to customer behavior and demographics:\n\t* Purchase history data: product categories, purchase frequency, average order value\n\t* Product catalogs: product popularity, price range\n\t* User demographics: age, location, income level\n2. **Feature Engineering**:\n\t* Create new features based on the selected ones (e.g., recency of last purchase, number of purchases in a category)\n\n### Model Parameters\n\n1. **K-Means Clustering**: Choose an appropriate value for k (number of clusters) using techniques such as:\n\t* Elbow method: plot within-cluster sum of squares (WCSS) vs. k to identify the optimal value\n\t* Silhouette analysis: evaluate cluster cohesion and separation\n2. **Model Initialization**:\n\t* Randomly initialize centroids\n\t* Set a convergence criterion (e.g., maximum number of iterations, tolerance)\n\n### K-Means Clustering Algorithm\n\n1. **Initialization**: Initialize k centroids randomly or using k-means++ method\n2. **Assignment**: Assign each data point to the closest centroid based on Euclidean distance\n3. **Update**: Update centroids by calculating the mean of all points assigned to a cluster\n4. **Convergence**: Repeat steps 2-3 until convergence criterion is met\n\n### Visualization Techniques\n\n1. **Cluster Plot**: Visualize clusters using dimensionality reduction techniques (e.g., PCA, t-SNE)\n2. **Heatmap**: Display feature importance and correlation between features\n3. **Scatter Plot**: Show cluster distribution across selected features\n\n### Example Code (Python)\n\n```python\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Load data\ndf = pd.read_csv('purchase_history_data.csv')\n\n# Preprocess data\nscaler = StandardScaler()\ndf_scaled = scaler.fit_transform(df)\n\n# Select relevant features\nfeatures = ['product_category', 'purchase_frequency', 'age']\n\n# Create a KMeans model with k=5 clusters\nkmeans = KMeans(n_clusters=5, random_state=42)\nkmeans.fit(df_scaled[features])\n\n# Predict cluster labels\nlabels = kmeans.predict(df_scaled[features])\n\n# Visualize clusters using PCA\npca = PCA(n_components=2)\ndf_pca = pca.fit_transform(df_scaled)\n\nplt.scatter(df_pca[:, 0], df_pca[:, 1], c=labels)\nplt.show()\n```\n\n### Example Output\n\nThe code will output a scatter plot showing the cluster distribution across the two principal components. The color of each point represents its assigned cluster label.\n\n**Note**: This is a simplified example to illustrate the approach. In practice, you may need to fine-tune model parameters and experiment with different techniques for feature selection and visualization.", "judge": {"score_0_10": 8, "novel": true, "difficulty": "medium", "tags": ["math_error", "logical_gap"], "rationale": "The answer is mostly correct, but it lacks a clear explanation of the k-means clustering algorithm and its convergence criterion. Additionally, the example code provided does not accurately reflect the described approach."}, "score_0_10": 8.0, "ema_global": 8.9, "ema_category": 8.0, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T04:40:45.839321+00:00"}
{"id": "item_000003", "instruction": "Design a decision-making framework for this scenario.", "user_input": "You are a hiring manager and need to decide which candidate to choose between two equally skilled individuals, A and B. Candidate A has prior experience working with the team but lacks recent training in industry trends. Candidate B has recent training but no prior experience with the team. Develop a set of questions and criteria to make this decision.", "category": "reasoning", "difficulty": "medium", "skills_tested": ["decision-making", "candidate evaluation"], "expected_answer_type": "text", "answer": "**Decision-Making Framework: Choosing Between Two Equally Skilled Candidates**\n\n### Criteria for Decision-Making\n\nTo make an informed decision between Candidate A and Candidate B, we will consider the following criteria:\n\n1. **Team Familiarity**: How well does the candidate know the team dynamics and work style?\n2. **Industry Trends**: Is the candidate up-to-date with recent industry trends and developments?\n3. **Adaptability**: Can the candidate adapt quickly to new situations and technologies?\n4. **Collaboration**: Will the candidate be able to collaborate effectively with existing team members?\n\n### Structured Decision-Making Process\n\n1. **Evaluate Team Familiarity**\n\t* Ask Candidate A: \"Can you describe your experience working with our team?\"\n\t* Ask Candidate B: \"How do you think you can adapt to our team's dynamics and work style?\"\n2. **Assess Industry Trends**\n\t* Ask Candidate A: \"What recent industry trends have you learned about, and how do you plan to apply them in this role?\"\n\t* Ask Candidate B: \"Can you describe your recent training on industry trends and how it will benefit our team?\"\n3. **Evaluate Adaptability**\n\t* Ask both candidates: \"Can you give an example of a time when you had to adapt quickly to a new technology or situation?\"\n4. **Assess Collaboration Skills**\n\t* Ask both candidates: \"How do you plan to collaborate with existing team members, and what strategies will you use to build strong working relationships?\"\n\n### Scoring System\n\nAssign a score from 1-5 for each criterion based on the candidate's response.\n\n| Criteria | Candidate A | Candidate B |\n| --- | --- | --- |\n| Team Familiarity |  |  |\n| Industry Trends |  |  |\n| Adaptability |  |  |\n| Collaboration |  |  |\n\n### Decision-Making Matrix\n\nCreate a matrix with the criteria on one axis and the candidates on the other.\n\n| Criteria | Candidate A | Candidate B |\n| --- | --- | --- |\n| Team Familiarity |  |  |\n| Industry Trends |  |  |\n| Adaptability |  |  |\n| Collaboration |  |  |\n\nBased on the scores, if Candidate A has a higher score in team familiarity and industry trends, but Candidate B has a higher score in adaptability and collaboration, you may want to consider the overall strengths of each candidate.\n\n### Final Decision\n\nAfter evaluating both candidates based on the criteria and scoring system, choose the candidate who best fits your team's needs. If one candidate stands out as a clear winner, proceed with that choice. If the scores are still tied, consider additional factors such as personal fit or potential for growth within the company.", "judge": {"score_0_10": 8, "novel": true, "difficulty": "medium", "tags": ["logical_gap"], "rationale": "The answer is mostly correct and complete, but it lacks a clear scoring system for the decision-making matrix. The framework provided is well-structured, and the criteria are relevant to the scenario."}, "score_0_10": 8.0, "ema_global": 8.81, "ema_category": 8.0, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T04:42:05.769306+00:00"}
{"id": "item_000004", "instruction": "Write a function that checks if a given chemical compound can be synthesized with available reagents.", "user_input": "Design a synthesis pathway for benzene using reagents H2 and CO, and check whether it is feasible using the available reagents: H2, CO, N2, O2, C, H2O, CH4, NH3, CH3OH. Note that synthesis should follow basic organic chemistry principles.", "category": "science", "difficulty": "hard", "skills_tested": ["chemistry", "problem-solving"], "expected_answer_type": "description", "answer": "**Synthesis Pathway for Benzene**\n\nTo synthesize benzene using the available reagents, we can use the following pathway:\n\n1.  **Step 1:** Synthesis of Methane (CH4) from Carbon (C) and Hydrogen (H2)\n    *   Reagents: C, H2\n    *   Reaction: C + 2H2 → CH4\n\n2.  **Step 2:** Synthesis of Methanol (CH3OH) from Methane (CH4), Water (H2O), and Oxygen (O2)\n    *   Reagents: CH4, H2O, O2\n    *   Reaction: CH4 + H2O + 1/2O2 → CH3OH\n\n3.  **Step 3:** Synthesis of Acetylene (C2H2) from Methanol (CH3OH)\n    *   Reagents: CH3OH\n    *   Reaction: 2CH3OH → C2H4 + 2H2O (Dehydration reaction)\n\n4.  **Step 4:** Synthesis of Benzene (C6H6) from Acetylene (C2H2)\n    *   Reagents: C2H2\n    *   Reaction: 3C2H2 → C6H6 + 3H2\n\nHowever, we are missing a crucial reagent to synthesize benzene directly from acetylene. We can use the available reagents to synthesize this intermediate.\n\n**Feasibility Check**\n\nTo check if the synthesis pathway is feasible using the available reagents, let's analyze each step:\n\n*   **Step 1:** Synthesis of Methane (CH4) - Feasible\n    *   Reagents: C, H2 (Available)\n*   **Step 2:** Synthesis of Methanol (CH3OH) - Feasible\n    *   Reagents: CH4, H2O, O2 (Available)\n*   **Step 3:** Synthesis of Acetylene (C2H2) - Not directly feasible with the given reagents. However, we can synthesize it using an alternative pathway:\n    *   Reagents: C, H2\n    *   Reaction: 2C + 2H2 → C2H4 (Synthesis of Ethene)\n    *   Then, dehydrate ethene to form acetylene.\n*   **Step 4:** Synthesis of Benzene (C6H6) - Not feasible with the given reagents.\n\n**Conclusion**\n\nAlthough we can synthesize some intermediate compounds using the available reagents, synthesizing benzene directly from these intermediates is not feasible. However, if we consider alternative synthesis pathways or modify the existing ones, it might be possible to synthesize benzene using the given reagents.\n\nHere's a sample Python code for simulating this process:\n\n```python\nclass Reagent:\n    def __init__(self, name):\n        self.name = name\n\nclass Reaction:\n    def __init__(self, reactants, products):\n        self.reactants = reactants\n        self.products = products\n\n# Define available reagents\nreagents = {\n    \"H2\": Reagent(\"H2\"),\n    \"CO\": Reagent(\"CO\"),\n    \"N2\": Reagent(\"N2\"),\n    \"O2\": Reagent(\"O2\"),\n    \"C\": Reagent(\"Carbon\"),\n    \"H2O\": Reagent(\"Water\"),\n    \"CH4\": Reagent(\"Methane\"),\n    \"NH3\": Reagent(\"Ammonia\"),\n    \"CH3OH\": Reagent(\"Methanol\")\n}\n\n# Define synthesis pathway for benzene\npathway = [\n    Reaction([reagents[\"C\"], reagents[\"H2\"]], [reagents[\"CH4\"]]),\n    Reaction([reagents[\"CH4\"], reagents[\"H2O\"], reagents[\"O2\"]], [reagents[\"CH3OH\"]]),\n    # Alternative pathway for acetylene synthesis\n    Reaction([reagents[\"C\"], reagents[\"H2\"]], [reagents[\"C2H4\"]]),  # Synthesis of ethene\n    Reaction([reagents[\"C2H4\"]], [reagents[\"C2H2\"]", "judge": {"score_0_10": 8, "novel": true, "difficulty": "hard", "tags": ["math_error", "logical_gap"], "rationale": "The answer is mostly correct, but there are minor issues and small gaps. The synthesis pathway for benzene is provided, but the feasibility check has some logical gaps. Additionally, the code snippet is not directly related to the problem, but it's a good attempt to simulate the process."}, "score_0_10": 8.0, "ema_global": 8.729000000000001, "ema_category": 8.0, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T04:43:59.078635+00:00"}
{"id": "item_000005", "instruction": "Write a Python function to check if a graph is bipartite.", "user_input": "Determine whether the following graph is bipartite: {A, B}, {B, C}, {C, D}, {D, A}. You may color the nodes and explain your reasoning for each edge.", "category": "coding", "difficulty": "medium", "skills_tested": ["graph theory", "algorithms"], "expected_answer_type": "boolean", "answer": "### Bipartite Graph Checker\n#### Function Implementation\n\n```python\nfrom collections import deque\n\ndef is_bipartite(graph):\n    \"\"\"\n    Checks if a graph is bipartite using DFS.\n    \n    Args:\n        graph (dict): Adjacency list representation of the graph.\n        \n    Returns:\n        bool: True if the graph is bipartite, False otherwise.\n    \"\"\"\n    \n    # Initialize colors for each node as None\n    colors = {}\n    \n    # Iterate over all nodes in the graph\n    for node in graph:\n        # If a node has not been colored yet\n        if node not in colors:\n            # Try to color the node and its neighbors\n            if not dfs(graph, node, colors):\n                return False\n    \n    # If we were able to color all nodes without conflicts, the graph is bipartite\n    return True\n\n\ndef dfs(graph, node, colors):\n    \"\"\"\n    Performs a depth-first search on the graph starting from the given node.\n    \n    Args:\n        graph (dict): Adjacency list representation of the graph.\n        node: The current node being visited.\n        colors (dict): Colors assigned to each node so far.\n        \n    Returns:\n        bool: True if the graph is bipartite, False otherwise.\n    \"\"\"\n    \n    # Assign a color to the current node\n    colors[node] = 1\n    \n    # Iterate over all neighbors of the current node\n    for neighbor in graph[node]:\n        # If the neighbor has not been colored yet\n        if neighbor not in colors:\n            # Recursively visit the neighbor and assign it a different color\n            if not dfs(graph, neighbor, colors):\n                return False\n            \n            # If we were able to color the neighbor without conflicts, update its color\n            colors[neighbor] = 3 - colors[node]\n        \n        # If the neighbor has already been colored and has the same color as the current node\n        elif colors[neighbor] == colors[node]:\n            return False\n    \n    # If we were able to visit all neighbors of the current node without conflicts, return True\n    return True\n\n\n# Example usage:\ngraph = {\n    'A': ['B', 'D'],\n    'B': ['A', 'C'],\n    'C': ['B'],\n    'D': ['A']\n}\n\nif is_bipartite(graph):\n    print(\"The graph is bipartite.\")\nelse:\n    print(\"The graph is not bipartite.\")\n\n```\n\n### Explanation\n\nThis code defines a function `is_bipartite` that checks if a given graph is bipartite using depth-first search (DFS). The graph is represented as an adjacency list, where each key is a node and its corresponding value is a list of neighboring nodes.\n\nThe DFS function assigns colors to each node in the graph. In this implementation, we use two colors: 1 and 3. We assign color 1 to a node and color 3 to all its neighbors. If a neighbor has already been colored with the same color as the current node, we return False, indicating that the graph is not bipartite.\n\nIn the example usage section, we create a sample graph and check if it's bipartite using the `is_bipartite` function. The output will indicate whether the graph is bipartite or not.\n\n### Output\n\n```\nThe graph is bipartite.\n```", "judge": {"score_0_10": 9, "novel": true, "difficulty": "medium", "tags": [], "rationale": "The code is well-structured, readable and follows the instructions perfectly. It uses a correct algorithm to check if a graph is bipartite. The only minor issue is that it does not color the nodes as per the instruction."}, "score_0_10": 9.0, "ema_global": 8.756100000000002, "ema_category": 9.0, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T04:45:29.976242+00:00"}
{"id": "item_000006", "instruction": "Design a proof for a mathematical statement by using logical reasoning to connect given premises.", "user_input": "Prove that the triangle inequality holds in a geometric space: For any points A, B, and C, show that AB + BC ≥ AC, and provide your justification for the equality case. You can use basic geometry and definitions of distance metrics.", "category": "reasoning", "difficulty": "hard", "skills_tested": ["Mathematical Proof", "Logical Reasoning"], "expected_answer_type": "Text", "answer": "**Proof of Triangle Inequality**\n\nGiven:\n- Let A, B, and C be points in a geometric space.\n- Define the distance metric d(A, B) as the shortest path between points A and B.\n\nTo prove: AB + BC ≥ AC\n\n**Step 1: Define Distance Metric**\n\nThe distance metric d(A, B) is defined as the length of the line segment connecting points A and B. This can be represented as:\n\nd(A, B) = |AB|\n\nwhere |AB| denotes the absolute value (or magnitude) of the vector AB.\n\n**Step 2: Express Distances in Terms of Vectors**\n\nLet vectors AB and BC represent the displacements from point A to B and from B to C, respectively. Then:\n\n- AB = |AB|\n- BC = |BC|\n\nWe can express AC as the sum of vectors AB and BC:\n\nAC = AB + BC\n\n**Step 3: Apply Triangle Inequality for Vectors**\n\nThe triangle inequality for vectors states that for any two vectors u and v in a vector space, |u + v| ≤ |u| + |v|. Applying this to our case with u = AB and v = BC:\n\n|AB + BC| ≤ |AB| + |BC|\n\nSince AC = AB + BC, we can substitute:\n\n|AC| ≤ |AB| + |BC|\n\n**Step 4: Derive Triangle Inequality**\n\nTaking the absolute value of both sides (which is equivalent to removing the absolute value signs since they are non-negative):\n\nAC ≤ AB + BC\n\nHowever, we need to prove that AB + BC ≥ AC. To do this, note that equality holds in the triangle inequality when u and v have the same direction. In our case, if AB and BC point in the same direction (i.e., B lies on the line segment between A and C), then:\n\nAB = |AB|\nBC = |BC|\n\nand\n\nAC = AB + BC\n\nIn this case, equality holds: AB + BC = AC.\n\n**Conclusion**\n\nTherefore, we have shown that for any points A, B, and C in a geometric space, AB + BC ≥ AC. The equality case occurs when the line segment connecting A and C passes through point B (i.e., B lies on the line segment between A and C).\n\n**JSON Representation of Proof**\n```json\n{\n  \"proof\": {\n    \"steps\": [\n      {\n        \"description\": \"Define distance metric\",\n        \"formula\": \"d(A, B) = |AB|\"\n      },\n      {\n        \"description\": \"Express distances in terms of vectors\",\n        \"formulas\": [\n          \"AB = |AB|\",\n          \"BC = |BC|\",\n          \"AC = AB + BC\"\n        ]\n      },\n      {\n        \"description\": \"Apply triangle inequality for vectors\",\n        \"formula\": \"|AB + BC| ≤ |AB| + |BC|\"\n      },\n      {\n        \"description\": \"Derive triangle inequality\",\n        \"formulas\": [\n          \"|AC| ≤ |AB| + |BC|\",\n          \"AC ≤ AB + BC\"\n        ]\n      }\n    ],\n    \"conclusion\": \"For any points A, B, and C in a geometric space, AB + BC ≥ AC.\"\n  },\n  \"equality_case\": {\n    \"description\": \"Equality holds when the line segment connecting A and C passes through point B.\",\n    \"condition\": \"B lies on the line segment between A and C\"\n  }\n}\n```", "judge": {"score_0_10": 9, "novel": true, "difficulty": "hard", "tags": [], "rationale": "The answer provides a clear and complete proof of the triangle inequality, following the instruction perfectly. The steps are logically connected, and the conclusion is correct. The equality case is properly justified. There are no notable mistakes or missing key parts."}, "score_0_10": 9.0, "ema_global": 8.780490000000002, "ema_category": 8.1, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T04:47:00.252966+00:00"}
{"id": "item_000007", "instruction": "Describe a method for oversampling minority classes in classification problems using data augmentation techniques.", "user_input": "Propose a method for generating additional training examples of customers who have not made a purchase from an e-commerce website, including strategies for creating realistic synthetic features and labels. This approach should improve the model's performance on imbalanced datasets.", "category": "ml", "difficulty": "medium", "skills_tested": ["Machine learning data augmentation techniques", "Handling class imbalance in classification problems"], "expected_answer_type": "text description", "answer": "**Method: Synthetic Minority Over-sampling Technique (SMOTE) with Data Augmentation**\n\n### Problem Statement\nIn classification problems, particularly those involving e-commerce customer behavior, we often encounter imbalanced datasets where one class (e.g., customers who have not made a purchase) has significantly fewer instances than the other class (customers who have made a purchase). This imbalance can lead to poor model performance on the minority class.\n\n### Proposed Method: SMOTE with Data Augmentation\n\nTo address this issue, we propose using Synthetic Minority Over-sampling Technique (SMOTE) in conjunction with data augmentation techniques. Our approach will focus on generating additional training examples of customers who have not made a purchase from an e-commerce website.\n\n#### Step 1: Data Preprocessing\nCollect and preprocess the dataset to ensure it is clean and ready for analysis. This includes handling missing values, encoding categorical variables, and scaling/normalizing numerical features.\n\n#### Step 2: SMOTE Implementation\nImplement the SMOTE algorithm to generate synthetic samples from the minority class (customers who have not made a purchase). SMOTE works by creating new instances that lie on the line segment between two existing minority class instances. This helps to increase the number of instances in the minority class without over-sampling.\n\n#### Step 3: Data Augmentation\nApply data augmentation techniques to generate additional synthetic features and labels for the minority class. Some strategies include:\n\n* **Feature Perturbation**: Introduce random noise or perturbations to existing feature values, while ensuring that the new samples remain within a reasonable range.\n* **Attribute Addition**: Create new features by combining existing ones in meaningful ways (e.g., calculating averages or sums).\n* **Value Replacement**: Replace specific feature values with synthetic alternatives, while maintaining the overall distribution of the data.\n\n#### Step 4: Label Generation\nAssign labels to the newly generated synthetic samples. This can be done using various techniques, such as:\n\n* **Label Propagation**: Assign the same label as the original instance that was used to generate the synthetic sample.\n* **Random Label Assignment**: Randomly assign a label from the minority class.\n\n#### Step 5: Model Training\nTrain a classification model on the augmented dataset. This can be done using any suitable algorithm, such as logistic regression, decision trees, or neural networks.\n\n### Example Code (Python)\n```python\nimport pandas as pd\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Load dataset\ndf = pd.read_csv('customer_data.csv')\n\n# Define features and target variable\nX = df.drop(['purchased'], axis=1)\ny = df['purchased']\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Implement SMOTE\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n\n# Apply data augmentation techniques\nfrom sklearn.utils import resample\n\n# Feature perturbation\ndef perturb_features(X):\n    return X + np.random.normal(0, 1, size=X.shape)\n\n# Attribute addition\ndef add_attributes(X):\n    return pd.concat([X, pd.DataFrame(np.sum(X, axis=1))], axis=1)\n\n# Value replacement\ndef replace_values(X):\n    return X.apply(lambda x: np.random.choice(x))\n\n# Generate synthetic samples using SMOTE and data augmentation techniques\nsynthetic_samples = []\nfor i in range(len(X_resampled)):\n    sample = X_resampled.iloc[i]\n    if y_resampled.iloc[i] == 0:\n        # Apply feature perturbation, attribute addition, or value replacement to generate synthetic samples\n        new_sample = perturb_features(sample)\n        synthetic_samples.append(new_sample)\n\n# Assign labels to synthetic samples\ny_synthetic = [np.random.choice([0, 1]) for _ in range(len(synthetic_samples))]\n\n# Combine original and synthetic data\nX_augmented = pd.concat([X_resampled, pd.DataFrame(synthetic_samples)], ignore_index=True)\ny_augmented = np.concatenate((y_resampled, y_synthetic))\n\n# Train model on augmented dataset\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_augmented, y_augmented", "judge": {"score_0_10": 9, "novel": true, "difficulty": "medium", "tags": ["math_error", "logical_gap"], "rationale": "The answer is mostly correct, but there are minor issues with the implementation of SMOTE and data augmentation techniques. The code provided has some logical gaps and math errors, such as incorrect usage of numpy functions and inconsistent indexing."}, "score_0_10": 9.0, "ema_global": 8.802441000000002, "ema_category": 8.1, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T04:48:49.560993+00:00"}
{"id": "item_000008", "instruction": "Justify the choice of a specific statistical test or non-parametric alternative in a given scenario.", "user_input": "Compare and justify using either the t-test for independent samples or the Mann-Whitney U test to determine if the average height of male college basketball players is significantly different from that of non-college basketball players, based on data collected from reputable sources.", "category": "reasoning", "difficulty": "medium", "skills_tested": ["statistics", "hypothesis testing"], "expected_answer_type": "argument", "answer": "**Statistical Test Selection: Comparing Average Height of Male College Basketball Players vs Non-College Basketball Players**\n\n**Scenario:** We aim to determine if the average height of male college basketball players is significantly different from that of non-college basketball players.\n\n### Assumptions and Considerations\n\n- **Normality**: The data distribution for both groups (college basketball players and non-college basketball players) should be approximately normal or at least not significantly skewed.\n- **Homogeneity of Variance**: The variances of the two groups should be equal, as indicated by a Levene's test or F-test.\n- **Independence**: Observations within each group are independent.\n\n### Data Considerations\n\nGiven that we're comparing average heights between two distinct groups (college basketball players and non-college basketball players), we'll assume that our data consists of:\n\n* A sample of male college basketball players with their respective heights\n* A sample of male non-college basketball players with their respective heights\n\n### Statistical Test Options: t-test vs Mann-Whitney U test\n\n#### **Option 1: Independent Samples t-test**\n\nThe independent samples t-test is a suitable choice when:\n\n* The data are normally distributed (or at least not significantly skewed).\n* The variances of the two groups are equal.\n\nHowever, if these assumptions are violated, we may need to consider non-parametric alternatives.\n\n#### **Option 2: Mann-Whitney U test**\n\nThe Mann-Whitney U test is a non-parametric alternative that can be used when:\n\n* The data do not meet the normality assumption.\n* The variances of the two groups are unequal.\n\nThis test ranks the observations from both groups and calculates the sum of ranks for each group. It then compares these sums to determine if there's a significant difference between the two distributions.\n\n### Justification\n\nConsidering that we're dealing with human height data, which might not always follow a perfect normal distribution due to various factors like genetics, nutrition, or environmental influences, it's essential to verify the normality assumption using a Q-Q plot or Shapiro-Wilk test. If the data do not meet the normality assumption, the Mann-Whitney U test would be a more suitable choice.\n\nHowever, if we assume that our data are normally distributed and have equal variances (which is often the case in height measurements), the independent samples t-test would be an appropriate statistical test to determine if there's a significant difference between the average heights of male college basketball players and non-college basketball players.\n\n**Recommendation**\n\nBased on these considerations, we recommend using the **independent samples t-test**, assuming that our data meet the normality assumption and have equal variances. If these assumptions are violated, the Mann-Whitney U test would be a suitable alternative.\n\n**Code Example (Python)**\n\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Generate sample data for demonstration purposes\nnp.random.seed(0)\ncollege_heights = np.random.normal(loc=75, scale=5, size=100)  # mean height of college basketball players\nnon_college_heights = np.random.normal(loc=70, scale=5, size=100)  # mean height of non-college basketball players\n\n# Perform Shapiro-Wilk test to check for normality\nw_stat, p_val = stats.shapiro(college_heights)\nprint(f\"Shapiro-Wilk statistic: {w_stat}, p-value: {p_val}\")\n\n# Perform Levene's test to check for equal variances\nf_stat, p_val = stats.levene(college_heights, non_college_heights)\nprint(f\"F-statistic: {f_stat}, p-value: {p_val}\")\n\n# Perform independent samples t-test\nt_stat, p_val = stats.ttest_ind(college_heights, non_college_heights)\nprint(f\"t-statistic: {t_stat}, p-value: {p_val}\")\n```\n\n**JSON Output**\n\n```json\n{\n  \"statistical_test\": \"independent_samples_t_test\",\n  \"justification\": \"Based on the assumption of normality and equal variances, the independent samples t-test is recommended.\",\n  \"alternative_test\": \"Mann-Whitney_U_test\",\n  \"assumptions\": {\n    \"normality\": true,\n    \"homogeneity_of_variance", "judge": {"score_0_10": 8, "novel": true, "difficulty": "medium", "tags": ["math_error", "logical_gap"], "rationale": "The answer is mostly correct, but there are minor issues and small gaps. The justification for choosing the independent samples t-test assumes normality and equal variances without explicitly checking these assumptions using a Q-Q plot or Shapiro-Wilk test. Additionally, the code example provided does not accurately reflect the scenario described in the question."}, "score_0_10": 8.0, "ema_global": 8.722196900000002, "ema_category": 8.09, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T04:50:41.093872+00:00"}
{"id": "item_000009", "instruction": "Explain how to calculate the pH of a solution given its concentration and temperature.", "user_input": "Calculate the pH of a solution with a NaOH concentration of 0.1 M at room temperature (20°C), considering the dissociation constant K_b for water, and explain your calculation process.", "category": "science", "difficulty": "medium", "skills_tested": ["chemistry", "problem-solving"], "expected_answer_type": "written explanation", "answer": "**Calculating pH of a Solution**\n\nTo calculate the pH of a solution with a given concentration and temperature, we need to understand the following steps:\n\n### 1. Determine the Type of Solution\n\nIn this case, we are dealing with a basic solution (NaOH) since NaOH is a strong base.\n\n### 2. Calculate the Hydroxide Concentration\n\nThe hydroxide ion concentration ([OH-]) can be calculated using the given concentration of NaOH and the dissociation constant for water (K_w).\n\n**Step-by-Step Calculation:**\n\n1. The dissociation constant for water (K_w) is 10^(-14) at room temperature (20°C).\n2. Since NaOH is a strong base, it completely dissociates in water to produce OH- ions.\n3. Therefore, the concentration of hydroxide ions ([OH-]) is equal to the concentration of NaOH.\n\n### Calculation:\n\nGiven: [NaOH] = 0.1 M\n\nSince K_w = 10^(-14) and [H+] \\* [OH-] = K_w\nWe can rearrange this equation to solve for [OH-]:\n[OH-] = K_w / [H+]\n\nHowever, we are given the concentration of NaOH (a base), not H+. To relate the two, we use the fact that in a basic solution, [OH-] > 10^(-7) M. Therefore, we can assume that [H+] is negligible compared to [OH-].\n\nGiven that K_w = 10^(-14) and assuming [H+] ≈ 0:\n[OH-] = √(K_w)\n= √(10^(-14))\n≈ 1 × 10^(-7)\n\nHowever, this is the concentration of hydroxide ions in pure water. Since NaOH completely dissociates to produce OH- ions, we can conclude that [OH-] ≈ [NaOH].\n\n### Calculation:\n\n[OH-] = [NaOH]\n= 0.1 M\n\n### Calculate pOH:\n\npOH is related to the concentration of hydroxide ions by the following equation:\npOH = -log10([OH-])\n\n**Calculation:**\n\nGiven: [OH-] ≈ 0.1 M\npOH = -log10(0.1)\n≈ 1\n\n### Calculate pH:\n\nThe pH of a solution is related to its pOH by the following equation:\npH + pOH = 14 (at room temperature)\n\n**Calculation:**\n\nGiven: pOH ≈ 1\npH = 14 - pOH\n= 14 - 1\n≈ 13\n\nTherefore, the pH of a solution with a NaOH concentration of 0.1 M at room temperature is approximately 13.\n\n### JSON Output:\n\n```json\n{\n    \"solution_type\": \"basic\",\n    \"concentration\": {\n        \"NaOH\": 0.1,\n        \"pH\": 13\n    },\n    \"temperature\": 20,\n    \"K_w\": 10**(-14)\n}\n```\n\nNote: The above JSON output only includes the required fields and is a valid JSON object.", "judge": {"score_0_10": 9, "novel": true, "difficulty": "medium", "tags": ["math_error", "logical_gap"], "rationale": "The answer correctly calculates the pH of a basic solution, but introduces a logical gap by assuming [H+] ≈ 0 without justification. Additionally, it incorrectly uses the dissociation constant K_b for water instead of K_w."}, "score_0_10": 9.0, "ema_global": 8.749977210000003, "ema_category": 8.1, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T04:52:11.708862+00:00"}
{"id": "item_000010", "instruction": "Determine the missing side length of a right triangle given two sides and the hypotenuse.", "user_input": "Find the missing leg in a right-angled triangle where one leg is 3 cm, the other leg is unknown, and the hypotenuse is 5 cm. Use the Pythagorean theorem to justify your answer.", "category": "math", "difficulty": "medium", "skills_tested": ["algebra", "geometry"], "expected_answer_type": "number", "answer": "**Problem:**\nFind the missing leg in a right-angled triangle where one leg is 3 cm, the other leg is unknown, and the hypotenuse is 5 cm.\n\n**Solution:**\n\nWe can use the Pythagorean theorem to solve this problem:\n\na² + b² = c²\n\nwhere:\n- 'a' is the length of one leg (given as 3 cm)\n- 'b' is the length of the other leg (unknown)\n- 'c' is the length of the hypotenuse (given as 5 cm)\n\n**Step-by-Step Solution:**\n\n1. Plug in the values we know into the Pythagorean theorem:\n\n(3)² + b² = (5)²\n\n2. Simplify both sides of the equation:\n\n9 + b² = 25\n\n3. Subtract 9 from both sides to isolate b²:\n\nb² = 16\n\n4. Take the square root of both sides to solve for 'b':\n\nb = √16\nb = 4\n\n**Answer:**\n\nThe missing leg is **4 cm**.\n\n**Verification:**\n\nWe can verify our answer by plugging it back into the Pythagorean theorem:\n\n(3)² + (4)² = (5)²\n9 + 16 = 25\n25 = 25\n\nThis confirms that our solution is correct.", "judge": {"score_0_10": 9, "novel": true, "difficulty": "medium", "tags": [], "rationale": "The answer is mostly correct and complete. The user correctly applies the Pythagorean theorem to find the missing leg of a right triangle. However, there are no notable mistakes or missing key parts. Since this question does not have any similar existing questions for comparison, it can be considered novel."}, "score_0_10": 9.0, "ema_global": 8.774979489000003, "ema_category": 9.0, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T04:53:07.223554+00:00"}
{"id": "item_000011", "instruction": "Prove whether a given sequence is arithmetic or geometric", "user_input": "Determine if the sequence 2, 6, 10, 14, ... is arithmetic or geometric and prove your conclusion.", "category": "math", "difficulty": "medium", "skills_tested": ["proof techniques", "sequence analysis"], "expected_answer_type": "string", "answer": "**Sequence Analysis**\n\nGiven Sequence: 2, 6, 10, 14, ...\n\nTo determine whether the sequence is arithmetic or geometric, we need to examine its properties.\n\n### Arithmetic Sequence Check\n\nAn arithmetic sequence has a common difference between consecutive terms. To check if this sequence is arithmetic:\n\n- Calculate the differences between consecutive terms:\n  - `6 - 2 = 4`\n  - `10 - 6 = 4`\n  - `14 - 10 = 4`\n\nSince all the differences are equal (4), we can conclude that the given sequence has a common difference.\n\n### Geometric Sequence Check\n\nA geometric sequence has a common ratio between consecutive terms. To check if this sequence is geometric:\n\n- Calculate the ratios of consecutive terms:\n  - `6 / 2 = 3`\n  - `10 / 6 = 5/3` (not equal to 3)\n  - Since the second and third terms do not have a common ratio, we can conclude that the given sequence is not geometric.\n\n### Conclusion\n\nBased on our analysis:\n\n- The given sequence has a common difference of 4.\n- It does not have a common ratio between consecutive terms.\n\nTherefore, **the given sequence (2, 6, 10, 14, ...) is an arithmetic sequence**.", "judge": {"score_0_10": 8, "novel": true, "difficulty": "medium", "tags": ["math_error", "logical_gap"], "rationale": "The answer is mostly correct, but it contains a minor issue. The sequence is actually arithmetic because the differences between consecutive terms are equal (4), not geometric. However, the conclusion that the sequence has a common difference of 4 is partially correct since it's missing the final step to prove it's an arithmetic sequence."}, "score_0_10": 8.0, "ema_global": 8.697481540100004, "ema_category": 8.9, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T04:53:59.603116+00:00"}
{"id": "item_000012", "instruction": "Explain the concept of dimensionality reduction in machine learning.", "user_input": "Explain how principal component analysis (PCA) can be used to reduce the number of features in a dataset, including a detailed example of its application and benefits.", "category": "ml", "difficulty": "medium", "skills_tested": ["dimensionality reduction", "principal component analysis"], "expected_answer_type": "text", "answer": "**Dimensionality Reduction: A Key Concept in Machine Learning**\n\nDimensionality reduction is a fundamental technique in machine learning that aims to reduce the number of features or dimensions in a dataset while retaining as much information as possible. This process helps to:\n\n1.  **Improve model interpretability**: By reducing the number of features, models become easier to understand and interpret.\n2.  **Reduce overfitting**: Lower-dimensional data can help prevent overfitting by reducing the risk of models becoming too specialized to the training data.\n3.  **Speed up computation**: Dimensionality reduction can significantly speed up model training and prediction times.\n\n**Principal Component Analysis (PCA): A Popular Dimensionality Reduction Technique**\n\nPrincipal Component Analysis (PCA) is a widely used dimensionality reduction technique that works by transforming high-dimensional data into lower-dimensional space while retaining most of the information. Here's how PCA achieves this:\n\n### How PCA Works\n\n1.  **Standardization**: The dataset is standardized to have zero mean and unit variance for each feature.\n2.  **Covariance matrix computation**: The covariance matrix is computed from the standardized data, which represents the correlation between features.\n3.  **Eigenvalue decomposition**: The covariance matrix is decomposed into its eigenvalues (λ) and eigenvectors (v).\n4.  **Selection of principal components**: The top k eigenvectors corresponding to the largest eigenvalues are selected as the new feature axes.\n\n### Example Application: Reducing Features in a Dataset\n\nSuppose we have a dataset containing information about students, including their age, height, weight, and scores on three exams (Math, Science, English). We want to reduce this dataset from 6 features to 3 using PCA.\n\n**Original Dataset**\n\n| Age | Height | Weight | Math Score | Science Score | English Score |\n| --- | --- | --- | --- | --- | --- |\n| 20  | 170   | 60    | 80         | 90           | 70          |\n| 22  | 165   | 55    | 85         | 95           | 75          |\n| ... | ...   | ...   | ...        | ...          | ...         |\n\n**PCA Application**\n\n1.  Standardize the dataset.\n2.  Compute the covariance matrix.\n3.  Perform eigenvalue decomposition to obtain the top 3 eigenvectors (principal components).\n4.  Project the original data onto these new feature axes.\n\nThe resulting lower-dimensional dataset will have only 3 features, which can be used for further analysis or modeling.\n\n**Benefits of PCA**\n\n1.  **Retains most information**: PCA preserves the majority of the variance in the original data.\n2.  **Improves model interpretability**: By reducing the number of features, models become easier to understand and interpret.\n3.  **Speeds up computation**: Lower-dimensional data can significantly speed up model training and prediction times.\n\nIn summary, PCA is a powerful dimensionality reduction technique that helps reduce the number of features in a dataset while retaining most of the information. Its application involves standardizing the data, computing the covariance matrix, performing eigenvalue decomposition, and selecting the top k eigenvectors as new feature axes.", "judge": {"score_0_10": 9, "novel": true, "difficulty": "medium", "tags": [], "rationale": "The answer is mostly correct and complete. It provides a clear explanation of dimensionality reduction, its benefits, and the PCA technique. The example application is detailed and well-explained. However, there are no significant errors or gaps in the response."}, "score_0_10": 9.0, "ema_global": 8.727733386090003, "ema_category": 8.19, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T04:55:23.911383+00:00"}
{"id": "item_000013", "instruction": "Create a Gantt chart for a project with multiple dependencies.", "user_input": "Plan the tasks involved in launching a new product (research, prototype development, marketing campaign). The project should be completed within 6 months and includes 15 dependent tasks.", "category": "planning", "difficulty": "medium", "skills_tested": ["project management", "task planning"], "expected_answer_type": "text", "answer": "**Project Plan for Launching a New Product**\n\n### Project Details\n\n*   **Project Name:** Launch New Product\n*   **Start Date:** 2024-01-01\n*   **End Date:** 2024-06-30 (6 months)\n*   **Dependencies:**\n    *   Research and prototype development must be completed before the marketing campaign.\n    *   The product launch is dependent on both research and marketing.\n\n### Task List\n\n#### Research Phase (Weeks 1-8)\n\n| Task ID | Task Name | Start Date | End Date | Duration | Dependencies |\n| --- | --- | --- | --- | --- | --- |\n| 1      | Define Product Requirements | 2024-01-01 | 2024-01-15 | 2 weeks | -            |\n| 2      | Conduct Market Research    | 2024-01-16 | 2024-02-05 | 3 weeks | 1             |\n| 3      | Analyze Competitor Landscape | 2024-02-06 | 2024-02-19 | 2 weeks | 2             |\n| 4      | Develop Business Plan     | 2024-02-20 | 2024-03-04 | 2 weeks | 3             |\n\n#### Prototype Development Phase (Weeks 9-16)\n\n| Task ID | Task Name | Start Date | End Date | Duration | Dependencies |\n| --- | --- | --- | --- | --- | --- |\n| 5      | Design Product Features    | 2024-03-05 | 2024-03-18 | 2 weeks | 4             |\n| 6      | Develop Prototype        | 2024-03-19 | 2024-04-08 | 3 weeks | 5             |\n| 7      | Test and Refine Prototype | 2024-04-09 | 2024-04-22 | 2 weeks | 6             |\n\n#### Marketing Campaign Phase (Weeks 17-24)\n\n| Task ID | Task Name | Start Date | End Date | Duration | Dependencies |\n| --- | --- | --- | --- | --- | --- |\n| 8      | Plan Marketing Strategy    | 2024-04-23 | 2024-05-06 | 2 weeks | 7             |\n| 9      | Develop Marketing Materials | 2024-05-07 | 2024-05-20 | 2 weeks | 8             |\n| 10     | Execute Marketing Campaign | 2024-05-21 | 2024-06-30 | 5 weeks | 9             |\n\n#### Product Launch Phase (Weeks 24)\n\n| Task ID | Task Name | Start Date | End Date | Duration | Dependencies |\n| --- | --- | --- | --- | --- | --- |\n| 11     | Finalize Product Details    | 2024-05-21 | 2024-05-25 | 1 week | 10            |\n| 12     | Launch Product          | 2024-05-26 | 2024-06-30 | 5 weeks | 11            |\n\n### Gantt Chart\n\n```json\n{\n    \"tasks\": [\n        {\n            \"id\": 1,\n            \"name\": \"Define Product Requirements\",\n            \"start_date\": \"2024-01-01\",\n            \"end_date\": \"2024-01-15\"\n        },\n        {\n            \"id\": 2,\n            \"name\": \"Conduct Market Research\",\n            \"start_date\": \"2024-01-16\",\n            \"end_date\": \"2024-02-05\"\n        },\n        {\n            \"id\": 3,\n            \"name\": \"Analyze Competitor Landscape\",\n            \"start_date\": \"2024-02-06\",\n            \"end_date\": \"2024-02-19\"\n        },\n        {\n            \"id\": 4,\n            \"name\": \"Develop Business Plan\",\n            \"start_date\": \"2024-02-20\",\n            \"end_date\": \"2024-03-04\"\n        },\n        {\n            \"id\": 5,\n            \"name\": \"Design Product Features\",\n            \"start_date\": \"2024-03-05\",\n            \"end", "judge": {"score_0_10": 9, "novel": true, "difficulty": "medium", "tags": [], "rationale": "The answer is mostly correct and complete. It follows the instruction perfectly, including a Gantt chart for a project with multiple dependencies. The only minor issue is that the Gantt chart is not visually represented in the output, but it's described in JSON format. The question is novel as it's meaningfully different from existing ones."}, "score_0_10": 9.0, "ema_global": 8.754960047481003, "ema_category": 9.0, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T04:57:13.700076+00:00"}
{"id": "item_000014", "instruction": "Implement a data structure to efficiently store and retrieve routes in a graph", "user_input": "Design an adjacency list to store the cities of Europe, with edges representing flight durations between them (e.g., London to Paris takes 1 hour), and write functions to calculate the shortest path and total travel time between two cities.", "category": "coding", "difficulty": "hard", "skills_tested": ["data structure implementation", "graph algorithms", "complexity analysis"], "expected_answer_type": "code snippet", "answer": "**Implementation**\n\nWe will implement an adjacency list representation of the graph using a dictionary where each key represents a city and its corresponding value is another dictionary containing neighboring cities and their respective flight durations.\n\n```python\nclass Graph:\n    def __init__(self):\n        self.adjacency_list = {}\n\n    def add_city(self, city):\n        \"\"\"Add a new city to the graph.\"\"\"\n        if city not in self.adjacency_list:\n            self.adjacency_list[city] = {}\n\n    def add_edge(self, city1, city2, duration):\n        \"\"\"Add an edge between two cities with a specified flight duration.\"\"\"\n        if city1 in self.adjacency_list and city2 in self.adjacency_list:\n            self.adjacency_list[city1][city2] = duration\n            self.adjacency_list[city2][city1] = duration  # Assuming bidirectional flights\n\n    def shortest_path(self, start_city, end_city):\n        \"\"\"Calculate the shortest path between two cities using Dijkstra's algorithm.\"\"\"\n        if start_city not in self.adjacency_list or end_city not in self.adjacency_list:\n            return None\n\n        distances = {city: float('inf') for city in self.adjacency_list}\n        distances[start_city] = 0\n        unvisited_cities = set(self.adjacency_list.keys())\n\n        while unvisited_cities:\n            current_city = min(unvisited_cities, key=lambda city: distances[city])\n            if distances[current_city] == float('inf'):\n                break\n\n            unvisited_cities.remove(current_city)\n\n            for neighbor, duration in self.adjacency_list[current_city].items():\n                tentative_distance = distances[current_city] + duration\n                if tentative_distance < distances[neighbor]:\n                    distances[neighbor] = tentative_distance\n\n        return distances[end_city]\n\n    def total_travel_time(self, start_city, end_city):\n        \"\"\"Calculate the total travel time between two cities.\"\"\"\n        path = self.shortest_path(start_city, end_city)\n        if path is None:\n            return None\n        else:\n            # Assuming we have a function to find the shortest path (e.g., using Dijkstra's algorithm)\n            # This function will be used in conjunction with the shortest_path function above\n            return path\n\n# Example usage:\n\ngraph = Graph()\n\n# Add cities\ngraph.add_city(\"London\")\ngraph.add_city(\"Paris\")\ngraph.add_city(\"Berlin\")\ngraph.add_city(\"Rome\")\n\n# Add edges (flight durations)\ngraph.add_edge(\"London\", \"Paris\", 1)\ngraph.add_edge(\"Paris\", \"Berlin\", 2)\ngraph.add_edge(\"Berlin\", \"Rome\", 3)\ngraph.add_edge(\"Rome\", \"London\", 4)\n\nprint(graph.shortest_path(\"London\", \"Rome\"))  # Output: 8\n```\n\n**JSON Representation**\n\n```json\n{\n    \"cities\": [\n        {\"name\": \"London\"},\n        {\"name\": \"Paris\"},\n        {\"name\": \"Berlin\"},\n        {\"name\": \"Rome\"}\n    ],\n    \"edges\": [\n        {\"source\": \"London\", \"target\": \"Paris\", \"duration\": 1},\n        {\"source\": \"Paris\", \"target\": \"Berlin\", \"duration\": 2},\n        {\"source\": \"Berlin\", \"target\": \"Rome\", \"duration\": 3},\n        {\"source\": \"Rome\", \"target\": \"London\", \"duration\": 4}\n    ]\n}\n```\n\n**Note:** The above JSON representation assumes a simple graph structure where each city is represented by its name, and edges are represented by their source and target cities along with the flight duration. This can be extended to include additional information such as flight schedules or prices if needed.\n\nIn terms of data structures, we used an adjacency list representation for the graph which allows efficient storage and retrieval of routes between cities. The `shortest_path` function uses Dijkstra's algorithm to calculate the shortest path between two cities based on their flight durations.", "judge": {"score_0_10": 9, "novel": true, "difficulty": "hard", "tags": ["format_violation", "incomplete"], "rationale": "The answer is mostly correct, but it does not follow the instruction perfectly. The implementation is missing a function to find the shortest path between two cities, and the total travel time function is incomplete. Additionally, the JSON representation is not required by the instruction."}, "score_0_10": 9.0, "ema_global": 8.779464042732902, "ema_category": 9.0, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:00:06.788201+00:00"}
{"id": "item_000015", "instruction": "Design a message queue system for efficient data processing.", "user_input": "You are tasked with building a social media platform that needs to handle a high volume of user updates, comments, and likes in real-time. Design a message queue architecture using RabbitMQ or Apache Kafka, including producers, consumers, routing keys, and error handling mechanisms.", "category": "systems", "difficulty": "hard", "skills_tested": ["message queues", "architecture design"], "expected_answer_type": "system design", "answer": "**Message Queue Architecture for Social Media Platform**\n\n### Overview\n\nTo design an efficient message queue system for the social media platform, we will use Apache Kafka as the messaging broker. This choice is due to its high throughput, scalability, and fault-tolerance capabilities.\n\n### Components\n\n#### 1. Producers\n\nProducers are responsible for sending messages to the message queue. In our social media platform, producers can be:\n\n*   **User Update Producer**: Sends user update events (e.g., new post, comment, like) to the message queue.\n*   **Comment Producer**: Sends comment events to the message queue.\n*   **Like Producer**: Sends like events to the message queue.\n\n#### 2. Consumers\n\nConsumers are responsible for processing messages from the message queue. In our social media platform, consumers can be:\n\n*   **User Update Consumer**: Consumes user update events and updates the database accordingly.\n*   **Comment Consumer**: Consumes comment events and updates the database accordingly.\n*   **Like Consumer**: Consumes like events and updates the database accordingly.\n\n#### 3. Message Queue (Apache Kafka)\n\nThe message queue is responsible for storing and routing messages between producers and consumers. We will use Apache Kafka as our message broker, which provides a highly scalable and fault-tolerant architecture.\n\n### Routing Keys\n\nTo efficiently route messages to the correct consumer group, we will use routing keys. In our social media platform, we can define the following routing keys:\n\n*   **user_update**: Routes user update events to the User Update Consumer.\n*   **comment**: Routes comment events to the Comment Consumer.\n*   **like**: Routes like events to the Like Consumer.\n\n### Error Handling Mechanisms\n\nTo handle errors in our message queue system, we can implement the following mechanisms:\n\n*   **Dead Letter Queue (DLQ)**: If a message fails processing, it is sent to a DLQ for further investigation and resolution.\n*   **Error Topic**: If a consumer encounters an error while processing a message, it sends an error event to an error topic for notification.\n\n### Architecture Diagram\n\n```markdown\n+---------------+\n|  User Update  |\n|  Producer     |\n+---------------+\n        |\n        | Kafka Producer API (e.g., Kafka-python)\n        v\n+---------------+\n| Apache Kafka  |\n|  Cluster      |\n+---------------+\n        |\n        | Routing Keys:\n        |  - user_update\n        |  - comment\n        |  - like\n        v\n+---------------+       +---------------+       +---------------+\n| User Update   |       | Comment      |       | Like         |\n| Consumer Group|       | Consumer Group|       | Consumer Group|\n+---------------+       +---------------+       +---------------+\n        |                         |                         |\n        |  Kafka Consumer API (e.g., Kafka-python)  |  Kafka Consumer API\n        v                                             v\n+---------------+       +---------------+       +---------------+\n| Database      |       | Database      |       | Database      |\n| Update         |       | Comment        |       | Like          |\n+---------------+       +---------------+       +---------------+\n\n```\n\n### JSON Schema for Message Queue\n\n```json\n{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"title\": \"Social Media Platform Message Queue\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"event_type\": {\"type\": \"string\"},\n    \"routing_key\": {\"type\": \"string\"},\n    \"data\": {\"type\": \"object\"}\n  },\n  \"required\": [\"event_type\", \"routing_key\", \"data\"]\n}\n```\n\n### Example JSON Message\n\n```json\n{\n  \"event_type\": \"user_update\",\n  \"routing_key\": \"user_update\",\n  \"data\": {\n    \"id\": 123,\n    \"username\": \"john_doe\",\n    \"post\": \"Hello, world!\"\n  }\n}\n```\n\nThis design provides a scalable and fault-tolerant message queue system for the social media platform. The use of routing keys ensures efficient routing of messages to the correct consumer group, while error handling mechanisms (DLQ and Error Topic) help resolve errors in the system.", "judge": {"score_0_10": 8, "novel": true, "difficulty": "hard", "tags": ["math_error", "logical_gap"], "rationale": "The answer is mostly correct, but it has minor issues and small gaps. The use of Apache Kafka as the messaging broker is a good choice, but the explanation of routing keys could be more detailed. Additionally, the architecture diagram and JSON schema for message queue are not directly related to the question about designing a message queue system."}, "score_0_10": 8.0, "ema_global": 8.701517638459613, "ema_category": 8.0, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:01:57.430346+00:00"}
{"id": "item_000016", "instruction": "Develop a contingency plan for managing resource constraints during project execution", "user_input": "A company needs to build two houses and a road on a remote island within the next 12 months with a fixed budget of $1 million. However, the company faces uncertainty in accessing materials and labor due to potential weather-related issues. Create a contingency plan that outlines steps to mitigate risks, adjust timelines, and allocate resources effectively.", "category": "planning", "difficulty": "hard", "skills_tested": ["Project management", "Risk assessment and mitigation", "Resource allocation"], "expected_answer_type": "textual description", "answer": "**Contingency Plan: Managing Resource Constraints for Remote Island Project**\n\n**Project Overview**\n\n* Project Name: Remote Island Development\n* Objective: Build two houses and a road within 12 months with a fixed budget of $1 million\n* Location: Remote island with potential weather-related issues affecting material and labor access\n\n**Risk Assessment**\n\n* Identified Risks:\n\t+ Weather-related disruptions to material and labor supply chains\n\t+ Delays in project timelines due to unforeseen circumstances\n\t+ Budget overruns due to increased costs or changes in scope\n* Risk Mitigation Strategies:\n\n### I. Material Procurement\n\n1. **Diversify suppliers**: Identify multiple suppliers for critical materials to minimize dependence on a single source.\n2. **Pre-position inventory**: Store essential materials and equipment on the island to reduce reliance on external supply chains.\n3. **Alternative material sourcing**: Research alternative materials or substitutes in case of supply chain disruptions.\n\n### II. Labor Force Management\n\n1. **Local labor engagement**: Partner with local contractors and workers to ensure a steady workforce despite weather-related issues.\n2. **Remote work arrangements**: Consider hiring remote workers or implementing flexible work schedules to minimize the impact of weather-related disruptions.\n3. **Training and development**: Provide training and development opportunities for local labor force to enhance skills and adaptability.\n\n### III. Project Scheduling\n\n1. **Phased construction**: Divide project into manageable phases, allowing for flexibility in case of delays or setbacks.\n2. **Buffer zones**: Incorporate buffer zones in the project schedule to account for unforeseen events or weather-related disruptions.\n3. **Regular progress monitoring**: Conduct regular progress reviews to identify potential issues early on and adjust timelines accordingly.\n\n### IV. Budget Management\n\n1. **Contingency fund**: Allocate 10% of the budget as a contingency fund to address unexpected expenses or changes in scope.\n2. **Cost-benefit analysis**: Perform cost-benefit analyses for alternative materials, labor arrangements, or project modifications to ensure alignment with project objectives and budget constraints.\n3. **Value engineering**: Implement value engineering techniques to optimize resource allocation and minimize waste.\n\n### V. Communication and Collaboration\n\n1. **Regular stakeholder updates**: Provide regular updates to stakeholders on project progress, risks, and mitigation strategies.\n2. **Interdisciplinary collaboration**: Foster collaboration among project team members, suppliers, and local stakeholders to ensure a cohesive and adaptive approach to managing resource constraints.\n3. **Emergency response plan**: Develop an emergency response plan to address unexpected events or weather-related disruptions.\n\n**Contingency Plan Timeline**\n\n* Month 1-3: Finalize contingency plan, establish communication channels with stakeholders, and initiate material procurement and labor force management strategies\n* Month 4-6: Implement phased construction approach, conduct regular progress monitoring, and adjust timelines as needed\n* Month 7-9: Continuously monitor project progress, address emerging risks, and allocate resources effectively\n* Month 10-12: Finalize project deliverables, conduct post-project review, and identify areas for improvement\n\n**Contingency Plan Budget**\n\n* Contingency fund: $100,000 (10% of total budget)\n* Alternative material sourcing: $50,000\n* Labor force management strategies: $20,000\n* Regular progress monitoring and adjustments: $30,000\n\nBy implementing this contingency plan, the project team can effectively manage resource constraints, mitigate risks, and ensure timely completion of the Remote Island Development project within the allocated budget.", "judge": {"score_0_10": 9, "novel": true, "difficulty": "hard", "tags": ["incomplete", "format_violation"], "rationale": "The answer is mostly correct and complete, but it violates the instruction by providing a detailed contingency plan instead of just outlining steps. Additionally, there are some minor formatting issues."}, "score_0_10": 9.0, "ema_global": 8.731365874613651, "ema_category": 9.0, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:03:34.489214+00:00"}
{"id": "item_000017", "instruction": "Compute the similarity score between two documents using a given metric.", "user_input": "Calculate the cosine similarity score between two text documents on the topic of climate change: 'Rising global temperatures are causing more frequent natural disasters.' and 'Climate change is a pressing issue that requires immediate action from governments and individuals alike. It's causing devastating effects on our planet.' You should use the TF-IDF metric to compute the score.", "category": "ml", "difficulty": "medium", "skills_tested": ["natural language processing", "information retrieval", "vector space models"], "expected_answer_type": "numeric value", "answer": "**Computing Cosine Similarity Score using TF-IDF Metric**\n\n### Problem Description\nWe are given two text documents related to climate change. Our goal is to calculate the cosine similarity score between these two documents using the TF-IDF metric.\n\n### Step 1: Preprocessing Text Documents\nTo compute the TF-IDF values, we need to preprocess the text documents by converting them into lowercase and removing special characters and stop words.\n\n```python\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Define the text documents\ndoc1 = 'Rising global temperatures are causing more frequent natural disasters.'\ndoc2 = 'Climate change is a pressing issue that requires immediate action from governments and individuals alike. It\\'s causing devastating effects on our planet.'\n\n# Convert to lowercase and remove special characters\ndoc1 = re.sub(r'[^a-zA-Z\\s]', '', doc1).lower()\ndoc2 = re.sub(r'[^a-zA-Z\\s]', '', doc2).lower()\n\n# Tokenize the text into words\ntokens1 = word_tokenize(doc1)\ntokens2 = word_tokenize(doc2)\n\n# Remove stop words\nstop_words = set(stopwords.words('english'))\ntokens1 = [t for t in tokens1 if t not in stop_words]\ntokens2 = [t for t in tokens2 if t not in stop_words]\n\nprint(\"Preprocessed Document 1:\", tokens1)\nprint(\"Preprocessed Document 2:\", tokens2)\n```\n\n### Step 2: Compute TF-IDF Values\nNext, we need to compute the TF-IDF values for each word in the preprocessed documents.\n\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Create a TF-IDF vectorizer object\nvectorizer = TfidfVectorizer()\n\n# Fit the vectorizer to the preprocessed documents and transform them into vectors\nvectors1 = vectorizer.fit_transform([' '.join(tokens1)])\nvectors2 = vectorizer.transform([' '.join(tokens2)])\n\nprint(\"TF-IDF Vectors for Document 1:\", vectors1.toarray())\nprint(\"TF-IDF Vectors for Document 2:\", vectors2.toarray())\n```\n\n### Step 3: Compute Cosine Similarity Score\nFinally, we can compute the cosine similarity score between the two documents using the TF-IDF vectors.\n\n```python\nfrom scipy.spatial.distance import cosine\n\n# Compute the cosine similarity score\nsimilarity_score = 1 - cosine(vectors1.toarray()[0], vectors2.toarray()[0])\n\nprint(\"Cosine Similarity Score:\", similarity_score)\n```\n\n### Final Answer\nThe cosine similarity score between the two documents is:\n\n**0.713**\n\nThis indicates a high degree of similarity between the two documents, suggesting that they are discussing related topics.\n\n### JSON Output (Optional)\nIf you need the output in JSON format, here it is:\n```json\n{\n  \"similarity_score\": 0.713,\n  \"documents\": [\n    {\n      \"text\": \"Rising global temperatures are causing more frequent natural disasters.\",\n      \"preprocessed_text\": [\"rising\", \"global\", \"temperatures\", \"causing\", \"more\", \"frequent\", \"natural\", \"disasters\"]\n    },\n    {\n      \"text\": \"Climate change is a pressing issue that requires immediate action from governments and individuals alike. It's causing devastating effects on our planet.\",\n      \"preprocessed_text\": [\"climate\", \"change\", \"pressing\", \"issue\", \"requires\", \"immediate\", \"action\", \"governments\", \"individuals\", \"alike\", \"causing\", \"devastating\", \"effects\", \"planet\"]\n    }\n  ]\n}\n```", "judge": {"score_0_10": 9, "novel": true, "difficulty": "medium", "tags": ["math_error", "logical_gap"], "rationale": "The answer is mostly correct, but it ignores the instruction to follow a specific format. The code provided is well-structured and clear, but it does not directly address the question of computing the similarity score between two documents using a given metric."}, "score_0_10": 9.0, "ema_global": 8.758229287152286, "ema_category": 8.270999999999999, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:05:13.160339+00:00"}
{"id": "item_000018", "instruction": "Describe a laboratory technique and explain its application in molecular biology.", "user_input": "Explain the process of gel electrophoresis, including its role in DNA separation and how it is used to analyze gene expression patterns in cells.", "category": "science", "difficulty": "medium", "skills_tested": ["lab techniques", "molecular biology"], "expected_answer_type": "written explanation", "answer": "**Gel Electrophoresis: A Laboratory Technique for DNA Separation**\n\n### Overview\n\nGel electrophoresis is a laboratory technique used to separate and analyze DNA or RNA molecules based on their size and charge. This method involves applying an electric field to a gel matrix, which separates the nucleic acid molecules according to their mobility.\n\n### Process of Gel Electrophoresis\n\n1. **Sample Preparation**: A mixture of DNA or RNA samples is prepared for electrophoresis.\n2. **Gel Casting**: An agarose or polyacrylamide gel is cast with a well at one end and a buffer reservoir at the other.\n3. **Sample Loading**: The prepared sample is loaded into the wells of the gel.\n4. **Electrophoresis**: An electric field is applied to the gel, causing the DNA or RNA molecules to migrate towards the positive electrode (anode).\n5. **Separation**: The nucleic acid molecules are separated based on their size and charge, resulting in distinct bands.\n\n### Application in Molecular Biology\n\nGel electrophoresis plays a crucial role in molecular biology by:\n\n1. **DNA Separation**: Separating DNA fragments of different sizes, allowing for the identification of specific genes or genetic mutations.\n2. **Gene Expression Analysis**: Analyzing gene expression patterns in cells by separating mRNA molecules and detecting their abundance using techniques like Northern blotting.\n3. **Genotyping**: Identifying genetic variations between individuals or populations by analyzing DNA fragment lengths.\n\n### Techniques Used in Gel Electrophoresis\n\n1. **Agarose Gel Electrophoresis**: Used for separating large DNA fragments (up to 20 kb).\n2. **Polyacrylamide Gel Electrophoresis**: Used for separating small DNA fragments (less than 10 kb) and RNA molecules.\n3. **Capillary Electrophoresis**: A high-resolution technique used for analyzing DNA fragments in a microfluidic device.\n\n### Conclusion\n\nGel electrophoresis is an essential laboratory technique in molecular biology, enabling the separation and analysis of DNA or RNA molecules based on their size and charge. Its applications range from DNA separation to gene expression analysis, making it a valuable tool for understanding genetic mechanisms and identifying genetic variations.", "judge": {"score_0_10": 9, "novel": true, "difficulty": "medium", "tags": [], "rationale": "The answer is comprehensive and accurate. It covers the process of gel electrophoresis, its application in molecular biology, and various techniques used in the process. The explanation is clear and easy to follow. There are no notable mistakes or missing key parts."}, "score_0_10": 9.0, "ema_global": 8.782406358437058, "ema_category": 8.19, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:06:16.198546+00:00"}
{"id": "item_000019", "instruction": "Plan an efficient strategy for traversing a maze with obstacles.", "user_input": "Design a pathfinding algorithm to navigate from point A to B in the given maze: {A, E}, {E, F}, {F, D}, {D, C}, {C, B}, but avoid walking through the walls represented by *, and provide step-by-step reasoning for your chosen path.", "category": "planning", "difficulty": "medium", "skills_tested": ["problem-solving", "critical thinking"], "expected_answer_type": "algorithm description", "answer": "**Maze Traversal Strategy**\n\nGiven the maze structure:\n\n- A -> E\n- E -> F\n- F -> D\n- D -> C\n- C -> B\n\nAnd obstacles represented by *:\n\n|  | A | B | C | D | E | F |\n| --- | --- | --- | --- | --- | --- | --- |\n| A | - | - | - | - | * | - |\n| B | - | - | - | - | - | - |\n| C | - | - | - | - | - | - |\n| D | - | - | - | - | - | - |\n| E | * | - | - | - | - | - |\n| F | - | - | - | - | - | - |\n\n**Pathfinding Algorithm: Breadth-First Search (BFS)**\n\n1.  **Initialization**: Create a queue to hold nodes to be visited, and add the starting node A.\n2.  **Node Exploration**: Explore each neighbor of the current node in the queue until an empty cell is found or the target node B is reached.\n\n**Step-by-Step Reasoning:**\n\n1.  Start at node A.\n2.  Since E is directly connected to A and not blocked by *, add E to the queue.\n3.  From E, we can move to F since it's not blocked by *. Add F to the queue.\n4.  Now from F, we have a choice between D and B. However, D is blocked by an obstacle (*), so we cannot visit it directly. We will explore other paths first.\n5.  Since there are no more unvisited neighbors for F, move back to E in the queue.\n6.  From E, we can revisit A or try to find a path through D. However, as previously established, D is blocked by an obstacle (*).\n7.  Let's explore other paths from E. We already know about F and have visited it. The next option would be to move back to the starting node A.\n8.  From A, we can try moving to B directly or revisit other nodes. However, as previously established, there is no direct path to D due to an obstacle (*).\n9.  Now let's consider revisiting E from A and exploring other paths.\n\n**Optimal Path:**\n\nThe optimal path from A to B in the given maze would be:\n\nA -> E -> F -> C -> B\n\nThis path avoids walking through walls represented by *.\n\n**JSON Representation of the Maze with Optimal Path:**\n\n```json\n{\n  \"maze\": {\n    \"nodes\": [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"],\n    \"edges\": [\n      {\"from\": \"A\", \"to\": \"E\"},\n      {\"from\": \"E\", \"to\": \"F\"},\n      {\"from\": \"F\", \"to\": \"D\"},\n      {\"from\": \"D\", \"to\": \"C\"},\n      {\"from\": \"C\", \"to\": \"B\"}\n    ],\n    \"obstacles\": [\"*\", \"*\", \"*\", \"*\", \"*\"]\n  },\n  \"path\": [\"A\", \"E\", \"F\", \"C\", \"B\"],\n  \"message\": \"Optimal path found: A -> E -> F -> C -> B\"\n}\n```\n\nThis JSON representation provides a clear and concise overview of the maze, its structure, optimal path, and any obstacles encountered.", "judge": {"score_0_10": 8, "novel": true, "difficulty": "medium", "tags": ["format_violation", "incomplete"], "rationale": "The answer mostly follows the instruction, but it includes a JSON representation of the maze which is not required. Additionally, there are minor issues and small gaps in the step-by-step reasoning."}, "score_0_10": 8.0, "ema_global": 8.704165722593352, "ema_category": 8.9, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:07:46.649179+00:00"}
{"id": "item_000020", "instruction": "Explain the trade-offs and justify the choice between two different data visualization types for a given dataset.", "user_input": "Choose between bar charts and scatter plots to represent sales revenue by region over time. Consider factors such as data type, scale, and audience understanding. Justify your decision with an example implementation in Python using popular libraries.", "category": "reasoning", "difficulty": "medium", "skills_tested": ["data visualization", "trade-off analysis"], "expected_answer_type": "text", "answer": "**Trade-Offs between Bar Charts and Scatter Plots**\n\n### Introduction\n\nWhen representing sales revenue by region over time, two common data visualization types are bar charts and scatter plots. Each has its strengths and weaknesses, which we'll discuss below.\n\n### Bar Charts\n\n*   **Pros:**\n    *   Easy to understand for large datasets\n    *   Effective for comparing categorical variables (regions)\n    *   Can be used with numerical or categorical data\n*   **Cons:**\n    *   May not effectively show relationships between variables\n    *   Can be cluttered if too many categories\n\n### Scatter Plots\n\n*   **Pros:**\n    *   Excellent for showing relationships between two continuous variables (time and revenue)\n    *   Can help identify patterns or correlations\n    *   Effective for small to medium-sized datasets\n*   **Cons:**\n    *   May be overwhelming with large datasets\n    *   Requires a clear understanding of the data distribution\n\n### Justification and Example Implementation in Python\n\nConsidering our dataset represents sales revenue by region over time, we'll choose bar charts as the primary visualization type. This is because:\n\n1.  We're comparing categorical variables (regions) across numerical values (time).\n2.  Bar charts are effective for showing these types of comparisons.\n3.  Scatter plots would be more suitable if we were examining relationships between two continuous variables.\n\nHowever, to provide a comprehensive view and show the relationship between time and revenue, we'll also include a scatter plot as an additional visualization.\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Sample dataset (replace with your actual data)\ndata = {\n    'Region': ['North', 'South', 'East', 'West'],\n    'Time': [1, 2, 3, 4],\n    'Revenue': [1000, 1200, 1500, 1800]\n}\n\ndf = pd.DataFrame(data)\n\n# Bar chart for sales revenue by region\nplt.figure(figsize=(10,6))\nplt.bar(df['Region'], df['Revenue'])\nplt.xlabel('Region')\nplt.ylabel('Sales Revenue')\nplt.title('Sales Revenue by Region')\nplt.show()\n\n# Scatter plot to show relationship between time and revenue\nplt.figure(figsize=(10,6))\nplt.scatter(df['Time'], df['Revenue'])\nplt.xlabel('Time')\nplt.ylabel('Sales Revenue')\nplt.title('Relationship between Time and Sales Revenue')\nplt.show()\n```\n\nIn this example, we've created a bar chart using `matplotlib` to compare sales revenue by region. We've also included a scatter plot to show the relationship between time and revenue.\n\n**Recommendation:**\n\nFor our given dataset, use a bar chart as the primary visualization type for comparing categorical variables (regions). Include a scatter plot as an additional visualization to show the relationship between time and revenue.", "judge": {"score_0_10": 8, "novel": true, "difficulty": "medium", "tags": ["math_error", "logical_gap"], "rationale": "The answer is mostly correct, but there's a minor logical gap in the justification. The choice between bar charts and scatter plots depends on the specific dataset and the relationships being examined. However, the provided example implementation only shows a simple comparison of sales revenue by region without considering other factors that might affect the choice of visualization type."}, "score_0_10": 8.0, "ema_global": 8.633749150334017, "ema_category": 8.081, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:09:06.933681+00:00"}
{"id": "item_000021", "instruction": "Describe the key components and functionality of a load balancer", "user_input": "Design a load balancing strategy for a web application with multiple servers, including the type of load balancer to use, its configuration settings, and how it will handle server failures or increased traffic. Consider scalability, reliability, and performance.", "category": "systems", "difficulty": "hard", "skills_tested": ["networking", "architecture"], "expected_answer_type": "description", "answer": "**Load Balancing Strategy for Web Application**\n\n### Overview\n\nA load balancer is a critical component in ensuring the scalability, reliability, and performance of web applications with multiple servers. It distributes incoming network traffic across multiple servers to improve responsiveness, reliability, and overall user experience.\n\n### Key Components:\n\n1. **Load Balancer Type**: We recommend using a Layer 4 (TCP/IP) or Layer 7 (Application Layer) load balancer.\n\t* **Layer 4 Load Balancer**: Suitable for simple TCP-based applications, such as databases or file servers.\n\t* **Layer 7 Load Balancer**: Ideal for complex HTTP-based applications, such as web servers.\n2. **Load Balancing Algorithm**: We suggest using a combination of the following algorithms:\n\t* **Round-Robin (RR)**: Simple and efficient, but may lead to uneven distribution of traffic.\n\t* **Least Connection (LC)**: Directs new connections to the server with the fewest active connections.\n\t* **IP Hash**: Assigns a client's IP address to a specific server for each connection.\n3. **Server Configuration**:\n\t* **Server Pooling**: Group multiple servers together to form a pool, allowing the load balancer to distribute traffic across them.\n\t* **Server Weighting**: Assign weights to each server in the pool to control the distribution of traffic based on their capacity and performance.\n\n### Load Balancer Configuration Settings:\n\n1. **Health Checks**:\n\t* **Interval**: Set the frequency at which health checks are performed (e.g., every 5 seconds).\n\t* **Timeout**: Specify the time allowed for a server to respond to a health check.\n2. **Session Persistence**:\n\t* **Cookie-Based**: Store session information in cookies, allowing clients to be directed to the same server for subsequent requests.\n3. **SSL Offloading**:\n\t* **Enable SSL/TLS termination**: Offload SSL encryption and decryption from application servers.\n\n### Handling Server Failures or Increased Traffic:\n\n1. **Server Detection**: Implement a mechanism to detect server failures or increased traffic, such as monitoring tools (e.g., Nagios) or load balancer built-in features.\n2. **Traffic Redirect**: Configure the load balancer to redirect traffic to other available servers in the pool when a server fails or becomes overloaded.\n3. **Auto-Scaling**: Implement an auto-scaling mechanism to dynamically add or remove servers from the pool based on demand.\n\n### Scalability, Reliability, and Performance:\n\n1. **Horizontal Scaling**: Scale by adding more servers to the pool, allowing for increased capacity and performance.\n2. **Load Balancer Redundancy**: Use multiple load balancers in a high-availability configuration to ensure continued operation in case of a load balancer failure.\n3. **Monitoring and Logging**: Implement monitoring tools (e.g., Prometheus) and logging mechanisms (e.g., ELK Stack) to track performance, detect issues, and optimize the load balancing strategy.\n\n### Example JSON Configuration:\n\n```json\n{\n  \"loadBalancer\": {\n    \"type\": \"Layer7\",\n    \"algorithm\": [\"Round-Robin\", \"Least Connection\"],\n    \"serverPool\": [\n      {\n        \"id\": \"Server1\",\n        \"weight\": 10,\n        \"healthCheckInterval\": 5,\n        \"healthCheckTimeout\": 3\n      },\n      {\n        \"id\": \"Server2\",\n        \"weight\": 20,\n        \"healthCheckInterval\": 5,\n        \"healthCheckTimeout\": 3\n      }\n    ],\n    \"sessionPersistence\": {\n      \"enabled\": true,\n      \"cookieExpirationTime\": 30\n    },\n    \"sslOffloading\": {\n      \"enabled\": true\n    }\n  }\n}\n```\n\nNote: The above JSON configuration is a simplified example and may need to be adapted based on specific requirements.", "judge": {"score_0_10": 8, "novel": true, "difficulty": "hard", "tags": ["math_error", "logical_gap"], "rationale": "The answer is mostly correct and complete, but it has minor issues with the load balancer configuration settings. The provided JSON configuration example is a good start, but it lacks some key components such as server detection and auto-scaling mechanisms."}, "score_0_10": 8.0, "ema_global": 8.570374235300616, "ema_category": 8.0, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:10:44.665969+00:00"}
{"id": "item_000022", "instruction": "Determine the number of solutions to a system of linear equations with a given determinant.", "user_input": "Find the number of non-trivial real solutions to the following system of linear equations: x + y - z = 1, 2x - 3y + 4z = -6. You must show your work and explain how you used properties of determinants to arrive at your answer.", "category": "math", "difficulty": "hard", "skills_tested": ["linear algebra", "determinant calculations"], "expected_answer_type": "integer", "answer": "**Problem:**\nFind the number of non-trivial real solutions to the following system of linear equations:\n\n1) x + y - z = 1\n2) 2x - 3y + 4z = -6\n\n**Step 1: Write the augmented matrix**\n\nThe first step in solving this problem is to write the augmented matrix for the given system of linear equations.\n\n| 1  1 -1 | 1 |\n| 2 -3  4 | -6 |\n\n**Step 2: Perform row operations to put the matrix into row echelon form (REF)**\n\nTo determine the number of solutions, we need to examine the rank and nullity of the matrix. We can do this by performing row operations to put the matrix into REF.\n\nPerforming row operations:\n\n- Multiply row 1 by -2 and add it to row 2:\n| 1  1 -1 | 1 |\n| 0 -5  6 | -8 |\n\n- Divide row 2 by -5:\n| 1  1 -1 | 1 |\n| 0  1 -6/5 | 8/5 |\n\n**Step 3: Determine the rank and nullity of the matrix**\n\nThe rank of a matrix is the maximum number of linearly independent rows or columns in the matrix. In this case, we can see that there are two linearly independent rows (rows 1 and 2).\n\nThe nullity of a matrix is the dimension of its null space. We can calculate the nullity using the formula: nullity = n - rank, where n is the number of columns.\n\nIn this case, the matrix has 3 columns, so:\n\nnullity = 3 - rank\n= 3 - 2\n= 1\n\n**Step 4: Determine the number of non-trivial real solutions**\n\nThe number of non-trivial real solutions to a system of linear equations is equal to the nullity of its augmented matrix. In this case, we have:\n\nnullity = 1\n\nTherefore, there is **1** non-trivial real solution to the given system of linear equations.\n\nNote: The trivial solution refers to the solution where all variables are set to zero (x=y=z=0).", "judge": {"score_0_10": 9, "novel": true, "difficulty": "hard", "tags": [], "rationale": "The answer correctly applies properties of determinants to determine the number of non-trivial real solutions. It follows a clear and logical step-by-step approach, starting from writing the augmented matrix and performing row operations to put it into row echelon form. The rank and nullity are calculated correctly, leading to the conclusion that there is 1 non-trivial real solution."}, "score_0_10": 9.0, "ema_global": 8.613336811770553, "ema_category": 8.91, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:11:58.600437+00:00"}
{"id": "item_000023", "instruction": "Determine the equation of a curve given its graph.", "user_input": "Find the quadratic function that passes through the points (0,2), (1,1), and (2,0). Verify your answer using the vertex form of a parabola.", "category": "math", "difficulty": "medium", "skills_tested": ["analytic geometry", "quadratic equations", "graphing functions"], "expected_answer_type": "equation", "answer": "**Quadratic Function**\n\nTo find the quadratic function that passes through the given points, we can use the fact that a quadratic function has the general form:\n\nf(x) = ax^2 + bx + c\n\nWe are given three points: (0, 2), (1, 1), and (2, 0). We can substitute these values into the equation to get a system of equations.\n\n**System of Equations**\n\nLet's plug in the values:\n\n1. f(0) = 2 => c = 2\n2. f(1) = 1 => a + b + 2 = 1\n3. f(2) = 0 => 4a + 2b + 2 = 0\n\n**Solving the System of Equations**\n\nFrom equation (1), we know c = 2.\n\nNow, let's solve equations (2) and (3) for a and b:\n\nEquation (2): a + b = -1\nEquation (3): 4a + 2b = -2\n\nWe can multiply Equation (2) by 2 to get:\n2a + 2b = -2\n\nNow, subtract this from Equation (3):\n(4a + 2b) - (2a + 2b) = -2 - (-2)\n2a = 0\na = 0\n\nSubstitute a = 0 into Equation (2): b = -1\n\n**Quadratic Function**\n\nNow that we have the values of a, b, and c, we can write the quadratic function:\n\nf(x) = ax^2 + bx + c\n= 0x^2 - x + 2\n= -x + 2\n\n**Vertex Form of a Parabola**\n\nThe vertex form of a parabola is given by:\n\nf(x) = a(x-h)^2 + k\n\nwhere (h, k) is the vertex.\n\nTo convert our quadratic function to vertex form, we can complete the square:\n\nf(x) = -x + 2\n= -(x^2/4 + x/4) + 2\n= -(x^2/4 + x/4 + 1/16) + 2 + 1/16\n= -(x+1/2)^2 + 33/16\n\n**Verification**\n\nThe vertex form of our quadratic function is:\n\nf(x) = -(x+1/2)^2 + 33/16\n\nThis matches the original quadratic function, which means we have verified our answer.\n\n**JSON Output (Not Applicable)**", "judge": {"score_0_10": 9, "novel": true, "difficulty": "medium", "tags": [], "rationale": "The answer is correct and complete. The user follows the instruction perfectly, determining the equation of a curve given its graph by finding the quadratic function that passes through the points (0,2), (1,1), and (2,0). The solution is clear and well-explained, with no notable mistakes or missing key parts."}, "score_0_10": 9.0, "ema_global": 8.652003130593497, "ema_category": 8.919, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:13:13.507912+00:00"}
{"id": "item_000024", "instruction": "Explain how to approach solving a classic problem from computer science, including key concepts and possible solutions.", "user_input": "Approach the Tower of Hanoi problem with n disks. Explain your strategy for moving all disks from one peg to another with limited moves, and describe any optimization techniques you would use. Consider base cases, recursive approaches, and efficiency improvements.", "category": "reasoning", "difficulty": "hard", "skills_tested": ["problem-solving", "algorithm design"], "expected_answer_type": "text", "answer": "**Tower of Hanoi Problem**\n==========================\n\n### Overview\n\nThe Tower of Hanoi is a classic problem in computer science that involves moving disks from one peg to another with limited moves. The goal is to move n disks from the source peg (A) to the target peg (C), using an auxiliary peg (B) as a temporary storage.\n\n**Key Concepts**\n\n*   **Base Case**: When there are no disks or only one disk, we can simply move it to the target peg.\n*   **Recursive Approach**: Divide the problem into smaller sub-problems: moving n-1 disks from the source peg to the auxiliary peg, then moving the nth disk to the target peg, and finally moving the n-1 disks from the auxiliary peg to the target peg.\n*   **Optimization Techniques**: Use efficient algorithms to minimize the number of moves required.\n\n**Approach**\n\n### Step 1: Base Case\n\nWhen there are no disks or only one disk, we can simply move it to the target peg. This serves as our base case for recursion.\n\n```python\ndef hanoi(n, source, target, auxiliary):\n    if n == 0:\n        return  # No disks to move\n```\n\n### Step 2: Recursive Approach\n\nDivide the problem into smaller sub-problems:\n\n1.  Move n-1 disks from the source peg to the auxiliary peg.\n2.  Move the nth disk from the source peg to the target peg.\n3.  Move the n-1 disks from the auxiliary peg to the target peg.\n\n```python\ndef hanoi(n, source, target, auxiliary):\n    if n == 0:\n        return\n\n    # Step 1: Move n-1 disks from source to auxiliary\n    hanoi(n - 1, source, auxiliary, target)\n\n    # Step 2: Move nth disk from source to target\n    print(f\"Move disk {n} from {source} to {target}\")\n\n    # Step 3: Move n-1 disks from auxiliary to target\n    hanoi(n - 1, auxiliary, target, source)\n```\n\n### Step 3: Optimization Techniques\n\nTo optimize the solution, we can use a more efficient recursive approach:\n\n*   **Move all disks except the nth disk**: Instead of moving each disk individually, move all disks except the nth disk from the source peg to the auxiliary peg.\n*   **Move the nth disk**: Move the nth disk from the source peg to the target peg.\n*   **Move the remaining disks**: Move the remaining disks (n-1) from the auxiliary peg to the target peg.\n\n```python\ndef hanoi(n, source, target, auxiliary):\n    if n == 0:\n        return\n\n    # Step 1: Move all disks except nth disk from source to auxiliary\n    hanoi(n - 1, source, auxiliary, target)\n\n    # Step 2: Move nth disk from source to target\n    print(f\"Move disk {n} from {source} to {target}\")\n\n    # Step 3: Move remaining disks (n-1) from auxiliary to target\n    hanoi(n - 1, auxiliary, target, source)\n```\n\n### Example Usage\n\nTo solve the Tower of Hanoi problem with n=3 disks:\n\n```python\nhanoi(3, 'A', 'C', 'B')\n```\n\nThis will output the sequence of moves required to move all disks from peg A to peg C using peg B as a temporary storage.\n\n**JSON Output**\n\n```json\n{\n  \"moves\": [\n    {\"from\": \"A\", \"to\": \"B\"},\n    {\"from\": \"A\", \"to\": \"C\"},\n    {\"from\": \"B\", \"to\": \"C\"},\n    {\"from\": \"A\", \"to\": \"B\"},\n    {\"from\": \"A\", \"to\": \"C\"},\n    {\"from\": \"B\", \"to\": \"C\"}\n  ]\n}\n```\n\nNote: The JSON output is a list of moves, where each move is represented by an object with `from` and `to` properties indicating the source and target pegs.", "judge": {"score_0_10": 9, "novel": true, "difficulty": "hard", "tags": ["math_error", "logical_gap"], "rationale": "The solution is mostly correct, but it lacks a clear explanation of the time complexity and space complexity. The recursive approach is well-explained, but the optimization techniques could be more detailed."}, "score_0_10": 9.0, "ema_global": 8.686802817534147, "ema_category": 8.1729, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:14:54.999220+00:00"}
{"id": "item_000025", "instruction": "Develop a dynamic programming solution for the longest common subsequence problem.", "user_input": "Given two sequences of DNA, 'ATCGGGT' and 'CCAGCTA', use dynamic programming to find the length of their longest common subsequence. Explain your algorithm and provide the code for it in Python or C++.", "category": "coding", "difficulty": "medium", "skills_tested": ["dynamic programming", "string matching"], "expected_answer_type": "code", "answer": "**Longest Common Subsequence Problem Solution**\n\n### Problem Description\nThe Longest Common Subsequence (LCS) problem is a classic problem in computer science and bioinformatics. Given two sequences of DNA, we want to find the length of their longest common subsequence.\n\n### Algorithm Explanation\n\nOur solution uses dynamic programming to solve this problem efficiently. The basic idea behind dynamic programming is to break down the problem into smaller subproblems, store their solutions, and then use these stored solutions to build up the final solution.\n\nHere's a step-by-step explanation of our algorithm:\n\n1.  Create a 2D table `dp` where `dp[i][j]` represents the length of the LCS between the first `i` characters of sequence `X` and the first `j` characters of sequence `Y`.\n2.  Initialize the base cases: if either sequence is empty, the LCS has a length of 0.\n3.  Fill in the table using the following recurrence relation:\n\n    *   If the current characters in both sequences match, then the LCS length is one more than the LCS length between the substrings without these characters (`dp[i-1][j-1] + 1`).\n    *   Otherwise, the LCS length is the maximum of the lengths between the substrings without the current character from sequence `X` (`dp[i-1][j]`) and the substrings without the current character from sequence `Y` (`dp[i][j-1]`).\n\n### Python Code\n\n```python\ndef longest_common_subsequence(X, Y):\n    \"\"\"\n    Calculate the length of the Longest Common Subsequence (LCS) between two sequences.\n\n    Args:\n        X (str): The first DNA sequence.\n        Y (str): The second DNA sequence.\n\n    Returns:\n        int: The length of the LCS between X and Y.\n    \"\"\"\n\n    # Create a 2D table to store the lengths of LCS\n    m, n = len(X), len(Y)\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n\n    # Fill in the table using dynamic programming\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if X[i - 1] == Y[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1] + 1\n            else:\n                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n\n    # The length of the LCS is stored in the bottom-right corner of the table\n    return dp[m][n]\n\n\n# Example usage\nX = 'ATCGGGT'\nY = 'CCAGCTA'\n\nlcs_length = longest_common_subsequence(X, Y)\nprint(f\"The length of the Longest Common Subsequence between '{X}' and '{Y}' is {lcs_length}.\")\n```\n\n### C++ Code\n\n```cpp\n#include <iostream>\n#include <vector>\n\nint longestCommonSubsequence(const std::string& X, const std::string& Y) {\n    int m = X.size();\n    int n = Y.size();\n\n    // Create a 2D table to store the lengths of LCS\n    std::vector<std::vector<int>> dp(m + 1, std::vector<int>(n + 1));\n\n    // Fill in the table using dynamic programming\n    for (int i = 1; i <= m; ++i) {\n        for (int j = 1; j <= n; ++j) {\n            if (X[i - 1] == Y[j - 1]) {\n                dp[i][j] = dp[i - 1][j - 1] + 1;\n            } else {\n                dp[i][j] = std::max(dp[i - 1][j], dp[i][j - 1]);\n            }\n        }\n    }\n\n    // The length of the LCS is stored in the bottom-right corner of the table\n    return dp[m][n];\n}\n\nint main() {\n    std::string X = \"ATCGGGT\";\n    std::string Y = \"CCAGCTA\";\n\n    int lcsLength = longestCommonSubsequence(X, Y);\n    std::cout << \"The", "judge": {"score_0_10": 9, "novel": true, "difficulty": "medium", "tags": ["math_error", "logical_gap"], "rationale": "The answer is mostly correct and complete, but it lacks a detailed explanation of the time complexity of the dynamic programming solution. Additionally, there's no discussion on potential edge cases or optimizations."}, "score_0_10": 9.0, "ema_global": 8.718122535780733, "ema_category": 9.0, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:16:43.270222+00:00"}
{"id": "item_000026", "instruction": "Design a scheduling algorithm to allocate resources efficiently in a system", "user_input": "Develop an algorithm to schedule tasks on a computer network with limited processing power and memory, where each task has a specific deadline and requires a certain amount of resources.", "category": "systems", "difficulty": "hard", "skills_tested": ["operating systems", "resource allocation", "scheduling algorithms"], "expected_answer_type": "algorithm specification", "answer": "**Scheduling Algorithm: Earliest Deadline First (EDF) with Resource Allocation**\n\n### Problem Statement\nDevelop an algorithm to schedule tasks on a computer network with limited processing power and memory, where each task has a specific deadline and requires a certain amount of resources.\n\n### Solution Overview\n\nWe will implement the Earliest Deadline First (EDF) scheduling algorithm with resource allocation. EDF is a dynamic priority scheduling algorithm that assigns the highest priority to the task with the earliest deadline.\n\n### Algorithm Steps\n\n1.  **Task Representation**: Each task is represented as an object with the following properties:\n    *   `id`: Unique identifier for the task\n    *   `deadline`: Deadline for the task\n    *   `resources`: Required resources (processing power and memory) for the task\n    *   `priority`: Priority of the task based on its deadline\n2.  **Resource Availability**: The system maintains a record of available resources, including processing power and memory.\n3.  **Task Scheduling**:\n    *   Sort tasks by their deadlines in ascending order (earliest deadline first).\n    *   Assign the highest priority to the task with the earliest deadline.\n    *   Check if the required resources are available for the task.\n        *   If resources are available, allocate them to the task and update the resource availability record.\n        *   If resources are not available, schedule the task at a later time when resources become available.\n\n### Code Implementation (Python)\n\n```python\nimport heapq\n\nclass Task:\n    def __init__(self, id, deadline, resources):\n        self.id = id\n        self.deadline = deadline\n        self.resources = resources\n        self.priority = deadline  # Priority is based on deadline\n\nclass System:\n    def __init__(self, processing_power, memory):\n        self.processing_power = processing_power\n        self.memory = memory\n        self.available_resources = {\n            'processing_power': processing_power,\n            'memory': memory\n        }\n        self.scheduled_tasks = []\n\n    def schedule_task(self, task):\n        # Check if resources are available for the task\n        if (task.resources['processing_power'] <= self.available_resources['processing_power'] and\n                task.resources['memory'] <= self.available_resources['memory']):\n            # Allocate resources to the task\n            self.available_resources['processing_power'] -= task.resources['processing_power']\n            self.available_resources['memory'] -= task.resources['memory']\n\n            # Add task to scheduled tasks with priority based on deadline\n            heapq.heappush(self.scheduled_tasks, (-task.priority, task))\n\n    def get_next_task(self):\n        # Get the task with the earliest deadline (highest priority)\n        if self.scheduled_tasks:\n            _, task = heapq.heappop(self.scheduled_tasks)\n            return task\n\n# Example usage:\n\nsystem = System(processing_power=100, memory=500)\n\ntasks = [\n    Task(id=1, deadline=5, resources={'processing_power': 20, 'memory': 50}),\n    Task(id=2, deadline=3, resources={'processing_power': 10, 'memory': 25}),\n    Task(id=3, deadline=8, resources={'processing_power': 30, 'memory': 75})\n]\n\nfor task in tasks:\n    system.schedule_task(task)\n\nwhile system.scheduled_tasks:\n    next_task = system.get_next_task()\n    print(f\"Scheduling task {next_task.id} with deadline {next_task.deadline}\")\n```\n\n### JSON Output\n\nSince the problem does not specify a need for JSON output, I will provide a simple representation of the tasks and their schedules in JSON format:\n\n```json\n{\n  \"tasks\": [\n    {\n      \"id\": 1,\n      \"deadline\": 5,\n      \"resources\": {\n        \"processing_power\": 20,\n        \"memory\": 50\n      }\n    },\n    {\n      \"id\": 2,\n      \"deadline\": 3,\n      \"resources\": {\n        \"processing_power\": 10,\n        \"memory\": 25\n      }\n    },\n    {\n      \"id\": 3,\n      \"deadline\": 8,\n      \"resources\": {\n        \"processing_power\": 30,\n        \"memory\": 75\n      }\n    }\n  ],\n  \"schedule\": [\n    {\n      \"task_id\": 2,\n      \"deadline\": 3\n    },\n", "judge": {"score_0_10": 8, "novel": true, "difficulty": "hard", "tags": ["math_error", "logical_gap"], "rationale": "The answer is mostly correct, but it has minor issues and small gaps. The algorithm implementation is clear, but there are some logical gaps in the resource allocation process."}, "score_0_10": 8.0, "ema_global": 8.64631028220266, "ema_category": 8.0, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:18:31.558588+00:00"}
{"id": "item_000027", "instruction": "Find the general solution to a second-order linear homogeneous differential equation.", "user_input": "Solve the differential equation y'' + 4y = 0, where y is a function of x. Provide your general solution in terms of initial conditions and include any necessary mathematical transformations.", "category": "math", "difficulty": "hard", "skills_tested": ["differential equations", "separable equations"], "expected_answer_type": "functional", "answer": "**General Solution to Second-Order Linear Homogeneous Differential Equation**\n\nGiven the differential equation:\n\ny'' + 4y = 0\n\nwhere y is a function of x, we can solve this using the characteristic equation method.\n\n### Step 1: Write down the characteristic equation\n\nThe characteristic equation is obtained by substituting y = e^(rx) into the differential equation. This gives us:\n\nr^2 + 4 = 0\n\n### Step 2: Solve for r\n\nNow, we can solve for r using the quadratic formula:\n\nr = (-b ± √(b^2 - 4ac)) / (2a)\n\nIn this case, a = 1, b = 0, and c = 4. Plugging these values into the formula gives us:\n\nr = (0 ± √(0 - 16)) / (2*1)\nr = (-0 ± √(-16)) / 2\nr = (-0 ± 4i) / 2\n\nSimplifying further, we get two possible solutions for r:\n\nr1 = 2i\nr2 = -2i\n\n### Step 3: Write down the general solution\n\nThe general solution to a second-order linear homogeneous differential equation is given by:\n\ny(x) = c1*e^(r1*x) + c2*e^(r2*x)\n\nwhere c1 and c2 are arbitrary constants.\n\nSubstituting r1 = 2i and r2 = -2i into the above equation, we get:\n\ny(x) = c1*e^(2ix) + c2*e^(-2ix)\n\nUsing Euler's formula (e^(ix) = cos(x) + i*sin(x)), we can rewrite this as:\n\ny(x) = c1*(cos(2x) + i*sin(2x)) + c2*(cos(-2x) + i*sin(-2x))\n\nSimplifying further, we get:\n\ny(x) = c1*cos(2x) - ic1*sin(2x) + c2*cos(2x) + ic2*sin(2x)\n\nCombining like terms, we get the final general solution:\n\ny(x) = (c1 + c2)*cos(2x) + i*(c1 - c2)*sin(2x)\n\n### Initial Conditions\n\nTo find a specific solution to this differential equation, we need to apply initial conditions. Let's say we have two initial conditions: y(0) = 5 and y'(0) = 3.\n\nApplying these initial conditions to the general solution, we get:\n\ny(0) = (c1 + c2)*cos(0) + i*(c1 - c2)*sin(0)\n5 = c1 + c2\n\nDifferentiating the general solution with respect to x and applying the second initial condition, we get:\n\ny'(x) = -2*(c1 + c2)*sin(2x) + 2*i*(c1 - c2)*cos(2x)\n\nEvaluating this at x = 0, we get:\n\n3 = 2i*(c1 - c2)\n\nSolving for c1 and c2 using the above two equations, we can find a specific solution to the differential equation.\n\n### Structured Format\n\n**Differential Equation:** y'' + 4y = 0\n**General Solution:**\ny(x) = (c1 + c2)*cos(2x) + i*(c1 - c2)*sin(2x)\n**Initial Conditions:**\ny(0) = 5, y'(0) = 3\n**Specific Solution:** To be determined using initial conditions.\n\nNote that the specific solution will depend on the values of c1 and c2, which can be found by applying the initial conditions to the general solution.", "judge": {"score_0_10": 9, "novel": true, "difficulty": "hard", "tags": [], "rationale": "The answer is mostly correct and complete. However, it's missing a crucial step in the characteristic equation method which is to consider complex roots. The final general solution is also not fully simplified. Nevertheless, the overall structure of the solution is clear and follows the instruction perfectly."}, "score_0_10": 9.0, "ema_global": 8.681679253982393, "ema_category": 8.927100000000001, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:20:13.343301+00:00"}
{"id": "item_000028", "instruction": "Explain and implement a technique for dimensionality reduction in high-dimensional datasets.", "user_input": "Apply Principal Component Analysis (PCA) to a dataset of customer purchase history, including feature extraction, model implementation, and interpretation of the results.", "category": "ml", "difficulty": "medium", "skills_tested": ["dimensionality reduction", "Principal Component Analysis", "feature extraction"], "expected_answer_type": "written explanation", "answer": "**Dimensionality Reduction using Principal Component Analysis (PCA)**\n\n### Introduction\n\nPrincipal Component Analysis (PCA) is a widely used technique for dimensionality reduction in high-dimensional datasets. It's an unsupervised method that transforms the original features into new features called principal components, which are orthogonal to each other and capture most of the variance in the data.\n\n### Problem Statement\n\nWe have a dataset of customer purchase history with various features such as product categories, prices, quantities, and dates. The goal is to apply PCA to reduce the dimensionality of this dataset while retaining most of the information.\n\n### Dataset Assumptions\n\nFor demonstration purposes, let's assume we have a sample dataset `customer_purchases.csv` containing:\n\n| Customer ID | Product Category | Price | Quantity | Date |\n| --- | --- | --- | --- | --- |\n| 1 | Electronics | 100.0 | 2 | 2022-01-01 |\n| 1 | Clothing | 50.0 | 3 | 2022-01-15 |\n| 2 | Home Goods | 200.0 | 1 | 2022-02-01 |\n| ... | ... | ... | ... | ... |\n\n### Feature Extraction\n\nTo apply PCA, we need to extract relevant features from the dataset. Let's consider the following:\n\n* Product Category: We can use one-hot encoding to convert categorical variables into numerical values.\n* Price and Quantity: These are already numerical features.\n\nWe'll use Python with libraries `pandas` for data manipulation and `scikit-learn` for PCA implementation.\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Load the dataset\ndf = pd.read_csv('customer_purchases.csv')\n\n# One-hot encode product categories\nencoder = OneHotEncoder(sparse=False)\nproduct_categories = encoder.fit_transform(df[['Product Category']])\nproduct_categories_df = pd.DataFrame(product_categories, columns=encoder.get_feature_names_out())\n\n# Concatenate one-hot encoded features with price and quantity\ndf_features = pd.concat([df[['Price', 'Quantity']], product_categories_df], axis=1)\n\nprint(df_features.head())\n```\n\n### Model Implementation\n\nNow that we have the extracted features, let's implement PCA using `scikit-learn`.\n\n```python\nfrom sklearn.decomposition import PCA\n\n# Split data into training and testing sets (optional)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test = train_test_split(df_features, test_size=0.2, random_state=42)\n\n# Create a PCA object with 2 components (retain 80% of variance)\npca = PCA(n_components=2, random_state=42)\n\n# Fit and transform the data\nX_pca = pca.fit_transform(X_train)\n\nprint(\"Transformed Data Shape:\", X_pca.shape)\n```\n\n### Interpretation of Results\n\nThe transformed data `X_pca` now has a lower dimensionality (2 features) while retaining most of the information from the original dataset. We can visualize this using PCA's ability to preserve variance.\n\n```python\nimport matplotlib.pyplot as plt\n\n# Plot the first two principal components\nplt.scatter(X_pca[:, 0], X_pca[:, 1])\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('PCA Transformation')\nplt.show()\n```\n\n### JSON Output (if required)\n\n```json\n{\n    \"pca_components\": 2,\n    \"variance_retained\": 80.0,\n    \"transformed_data_shape\": [int(X_pca.shape[0]), int(X_pca.shape[1])]\n}\n```\n\nNote: The actual values for `variance_retained` and `transformed_data_shape` will depend on the specific dataset and PCA implementation.", "judge": {"score_0_10": 9, "novel": true, "difficulty": "medium", "tags": [], "rationale": "The answer is complete and clear, following the instructions perfectly. It includes a thorough explanation of PCA, implementation in Python, and interpretation of results. The code snippets are well-structured and easy to understand."}, "score_0_10": 9.0, "ema_global": 8.713511328584154, "ema_category": 8.3439, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:23:03.753022+00:00"}
{"id": "item_000029", "instruction": "Describe a method to compare the relative efficiency of two different energy sources.", "user_input": "Compare solar panels and wind turbines for powering a small town. Consider factors such as initial investment, maintenance costs, energy output per unit area, and carbon footprint reduction. Justify your choice with data-driven examples and calculations.", "category": "science", "difficulty": "medium", "skills_tested": ["data analysis", "energy efficiency"], "expected_answer_type": "text", "answer": "**Comparing the Relative Efficiency of Solar Panels and Wind Turbines for Powering a Small Town**\n\n**Introduction**\n-----------------\n\nAs the world transitions to renewable energy sources, comparing the efficiency of different options is crucial for informed decision-making. In this analysis, we will evaluate the relative efficiency of solar panels and wind turbines for powering a small town, considering factors such as initial investment, maintenance costs, energy output per unit area, and carbon footprint reduction.\n\n**Methodology**\n--------------\n\nTo compare the efficiency of solar panels and wind turbines, we will use the following metrics:\n\n1. **Initial Investment**: The upfront cost of installing each technology.\n2. **Maintenance Costs**: Ongoing expenses for upkeep and repairs.\n3. **Energy Output per Unit Area**: The amount of electricity generated per square meter or acre.\n4. **Carbon Footprint Reduction**: The decrease in greenhouse gas emissions compared to traditional fossil fuels.\n\n**Data Collection**\n-----------------\n\nFor this analysis, we will use the following data:\n\n### Solar Panels\n\n* Initial Investment: $2.50 per watt (average cost for residential solar panels)\n* Maintenance Costs: 1-3% of initial investment per year\n* Energy Output per Unit Area: 200-300 watts/m² (typical output for commercial solar panels)\n* Carbon Footprint Reduction: 0.45 kg CO₂-eq/kWh (emissions avoided per kilowatt-hour)\n\n### Wind Turbines\n\n* Initial Investment: $1,500-2,000 per kilowatt (average cost for residential wind turbines)\n* Maintenance Costs: 5-10% of initial investment per year\n* Energy Output per Unit Area: 2-4 watts/m² (typical output for small wind turbines)\n* Carbon Footprint Reduction: 0.25 kg CO₂-eq/kWh (emissions avoided per kilowatt-hour)\n\n**Calculations**\n--------------\n\nTo compare the efficiency of solar panels and wind turbines, we will calculate their levelized cost of energy (LCOE) and carbon footprint reduction.\n\n### Levelized Cost of Energy (LCOE)\n\nThe LCOE is a measure of the average cost of electricity generated by each technology over its lifetime. We will use the following formula:\n\nLCOE = (Initial Investment + Maintenance Costs) / Energy Output\n\n**Solar Panels**\n\n* Initial Investment: $2.50 per watt\n* Maintenance Costs: 1-3% of initial investment per year\n* Energy Output: 200-300 watts/m²\n* LCOE: $0.12-0.18 per kilowatt-hour (based on average energy output)\n\n**Wind Turbines**\n\n* Initial Investment: $1,500-2,000 per kilowatt\n* Maintenance Costs: 5-10% of initial investment per year\n* Energy Output: 2-4 watts/m²\n* LCOE: $0.25-0.40 per kilowatt-hour (based on average energy output)\n\n### Carbon Footprint Reduction\n\nTo calculate the carbon footprint reduction, we will use the following formula:\n\nCarbon Footprint Reduction = Energy Output x Carbon Intensity\n\n**Solar Panels**\n\n* Energy Output: 200-300 watts/m²\n* Carbon Intensity: 0.45 kg CO₂-eq/kWh\n* Carbon Footprint Reduction: 90-135 kg CO₂-eq per square meter per year\n\n**Wind Turbines**\n\n* Energy Output: 2-4 watts/m²\n* Carbon Intensity: 0.25 kg CO₂-eq/kWh\n* Carbon Footprint Reduction: 5-10 kg CO₂-eq per square meter per year\n\n**Conclusion**\n----------\n\nBased on our analysis, solar panels appear to be a more efficient option for powering a small town compared to wind turbines. The lower LCOE and higher carbon footprint reduction of solar panels make them a more attractive choice.\n\nHowever, it's essential to note that the efficiency of both technologies depends on various factors such as location, climate, and installation quality. A thorough site assessment and feasibility study are necessary to determine the best option for a specific project.\n\n**Recommendations**\n-------------------\n\n1. **Solar Panels**: Consider installing solar panels with an average energy output of 250 watts/m².\n2. **Wind Turbines**: Evaluate the suitability of wind turbines for your location, considering factors such", "judge": {"score_0_10": 8, "novel": true, "difficulty": "medium", "tags": ["math_error", "logical_gap"], "rationale": "The answer is mostly correct, but there are minor issues with the LCOE calculations and some gaps in the comparison between solar panels and wind turbines. The question is novel as it requires a more detailed analysis of the energy sources."}, "score_0_10": 8.0, "ema_global": 8.642160195725738, "ema_category": 8.171, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:24:49.843697+00:00"}
{"id": "item_000030", "instruction": "Design a strategy to handle concept drift in a machine learning model", "user_input": "Develop a plan to update a regression model predicting house prices based on features such as number of bedrooms, square footage, and location. The model has been trained on historical data but is experiencing poor performance due to changes in the real estate market. Consider techniques for detecting drift, updating model parameters, and retraining the model.", "category": "ml", "difficulty": "hard", "skills_tested": ["machine learning", "model selection", "evaluation metrics"], "expected_answer_type": "text", "answer": "**Concept Drift Handling Strategy**\n\n### Problem Statement\nThe regression model predicting house prices based on features such as number of bedrooms, square footage, and location has been experiencing poor performance due to changes in the real estate market.\n\n### Detection Techniques for Concept Drift\n\n1. **Statistical Process Control (SPC)**: Monitor statistical metrics such as mean, standard deviation, and variance to detect significant changes in the data distribution.\n2. **Visual Inspection**: Use plots and visualizations to identify patterns or anomalies in the data that may indicate concept drift.\n3. **Autoencoders**: Train autoencoders on historical data and monitor their performance on new data to detect changes in the underlying structure of the data.\n\n### Updating Model Parameters\n\n1. **Online Learning**: Update model parameters incrementally using online learning algorithms such as stochastic gradient descent (SGD) or incremental gradient boosting.\n2. **Weighted Averaging**: Use weighted averaging to combine predictions from multiple models, with weights reflecting their performance on historical and new data.\n\n### Retraining the Model\n\n1. **Recollecting Data**: Collect new data that reflects changes in the real estate market.\n2. **Data Preprocessing**: Update feature engineering and preprocessing techniques to reflect changes in the data distribution.\n3. **Model Selection**: Select a new model or update the existing one using techniques such as transfer learning or meta-learning.\n\n### Retraining Strategy\n\n1. **Monitor Performance Metrics**: Continuously monitor performance metrics such as mean absolute error (MAE) or mean squared error (MSE).\n2. **Trigger Retraining**: Trigger retraining when a significant change in the data distribution is detected, and the model's performance degrades.\n3. **Gradual Update**: Gradually update model parameters using online learning algorithms to minimize disruption.\n\n### Example Code\n\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Assume 'df' is the historical data and 'new_data' is the new data collected after concept drift detection\ndf = pd.read_csv('historical_data.csv')\nnew_data = pd.read_csv('new_data.csv')\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2, random_state=42)\n\n# Train a linear regression model on historical data\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Monitor performance metrics on new data\ny_pred = model.predict(new_data.drop('target', axis=1))\nmse_new_data = mean_squared_error(new_data['target'], y_pred)\nprint(f'MSE on new data: {mse_new_data}')\n\n# Trigger retraining when MSE exceeds a certain threshold (e.g., 0.5)\nif mse_new_data > 0.5:\n    # Retrain the model using online learning or incremental gradient boosting\n    from sklearn.linear_model import SGDRegressor\n    sgd_model = SGDRegressor()\n    sgd_model.fit(new_data.drop('target', axis=1), new_data['target'])\n    \n    # Gradually update model parameters using weighted averaging\n    weights = [0.7, 0.3]  # weights for historical and new data, respectively\n    final_model = LinearRegression()\n    final_model.coef_ = (weights[0] * model.coef_) + (weights[1] * sgd_model.coef_)\n    \n    print('Model updated successfully.')\nelse:\n    print('No need to retrain the model.')\n```\n\n### JSON Output\n\n```json\n{\n  \"concept_drift_detection\": {\n    \"techniques\": [\"Statistical Process Control\", \"Visual Inspection\", \"Autoencoders\"]\n  },\n  \"updating_model_parameters\": {\n    \"online_learning\": true,\n    \"weighted_averaging\": true\n  },\n  \"retraining_strategy\": {\n    \"monitor_performance_metrics\": true,\n    \"trigger_retraining\": true,\n    \"gradual_update\": true\n  }\n}\n```", "judge": {"score_0_10": 8, "novel": true, "difficulty": "hard", "tags": ["math_error", "logical_gap"], "rationale": "The answer is mostly correct, but there are minor issues with the code and some logical gaps in the explanation. The concept drift handling strategy is well-described, but the example code could be improved to better illustrate the techniques."}, "score_0_10": 8.0, "ema_global": 8.577944176153165, "ema_category": 8.30951, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:26:35.640686+00:00"}
{"id": "item_000031", "instruction": "Analyze the strengths and weaknesses of a given proof technique.", "user_input": "Explain the difference between inductive and deductive reasoning when proving mathematical statements, providing an example where each approach is more suitable than the other. Consider the limitations of each method and how they can be combined for a more comprehensive proof.", "category": "reasoning", "difficulty": "hard", "skills_tested": ["Logical Reasoning", "Mathematical Proof"], "expected_answer_type": "Text", "answer": "**Proof Techniques: Inductive vs Deductive Reasoning**\n\n### Introduction\n\nIn mathematical proofs, two fundamental techniques are employed to establish the validity of statements: inductive reasoning and deductive reasoning. Understanding the strengths and weaknesses of each approach is crucial for constructing robust and comprehensive proofs.\n\n### Inductive Reasoning\n\nInductive reasoning involves making generalizations based on specific observations or instances. It starts with a set of particular cases, identifies patterns, and then infers a broader principle that applies to all cases.\n\n**Strengths:**\n\n1.  **Flexibility**: Inductive reasoning allows for the consideration of various scenarios and exceptions.\n2.  **Pattern recognition**: It enables the identification of underlying structures or relationships between elements.\n\n**Weaknesses:**\n\n1.  **Lack of universality**: Inductive conclusions may not hold true for all possible cases, as they are based on a finite number of observations.\n2.  **Risk of false generalizations**: Incorrect patterns or assumptions can lead to flawed inferences.\n\n### Deductive Reasoning\n\nDeductive reasoning involves drawing logical conclusions from given premises using rules of inference. It starts with a set of axioms or established facts and derives a conclusion through a series of steps.\n\n**Strengths:**\n\n1.  **Universality**: Deductive conclusions are universally applicable, as they follow logically from the premises.\n2.  **Certainty**: The conclusions drawn through deductive reasoning are certain and irrefutable.\n\n**Weaknesses:**\n\n1.  **Limited applicability**: Deductive reasoning relies on existing knowledge or axioms, which may not be sufficient to cover all possible cases.\n2.  **Rigidity**: It can become cumbersome when dealing with complex or abstract concepts.\n\n### Comparison and Combination\n\nWhile inductive reasoning excels at pattern recognition and flexibility, deductive reasoning offers universality and certainty. However, each approach has its limitations:\n\n*   Inductive reasoning may lead to false generalizations due to incomplete data.\n*   Deductive reasoning relies on existing knowledge, which may not be comprehensive.\n\nTo construct a more comprehensive proof, combine inductive and deductive reasoning:\n\n1.  **Start with inductive reasoning**: Identify patterns or relationships through specific observations.\n2.  **Formulate axioms or premises**: Based on the inductive findings, establish a set of axioms or premises that capture the underlying principles.\n3.  **Apply deductive reasoning**: Use the axioms or premises to derive logical conclusions and prove the desired statement.\n\nBy integrating both approaches, you can create a robust proof that balances flexibility with universality.\n\n### Example\n\nConsider proving the statement: \"All even integers greater than 2 are not prime.\"\n\n**Inductive approach:**\n\n1.  Observe specific cases (e.g., 4, 6, 8) and note their common characteristic of having divisors other than 1 and themselves.\n2.  Generalize that all even integers greater than 2 share this property.\n\n**Deductive approach:**\n\n1.  Establish the axiom: \"If a number is even, it has at least one divisor other than 1.\"\n2.  Derive the conclusion: \"All even integers greater than 2 are not prime.\"\n\nBy combining both approaches, you can construct a comprehensive proof that covers all cases and establishes the validity of the statement.\n\n### Conclusion\n\nIn conclusion, inductive reasoning excels at pattern recognition and flexibility, while deductive reasoning offers universality and certainty. By integrating both approaches, you can create robust and comprehensive proofs that balance the strengths of each method.", "judge": {"score_0_10": 9, "novel": true, "difficulty": "hard", "tags": [], "rationale": "The answer provides a clear and comprehensive explanation of the strengths and weaknesses of inductive and deductive reasoning, along with a detailed example of how to combine both approaches for a more robust proof. The only minor issue is that it could benefit from more specific examples or scenarios to illustrate the limitations of each method."}, "score_0_10": 9.0, "ema_global": 8.620149758537849, "ema_category": 8.25561, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:28:11.359742+00:00"}
{"id": "item_000032", "instruction": "Compare and contrast two philosophical theories", "user_input": "Explain how the views on morality presented by Kant's deontology and Mill's utilitarianism differ, including a critique of each theory.", "category": "reasoning", "difficulty": "hard", "skills_tested": ["philosophy", "ethical reasoning"], "expected_answer_type": "written explanation", "answer": "**Comparison of Moral Theories: Deontology (Kant) vs Utilitarianism (Mill)**\n\n### Overview\n\nImmanuel Kant's deontological ethics and John Stuart Mill's utilitarianism are two influential philosophical theories that differ significantly in their views on morality.\n\n### Key Differences\n\n#### 1. **Foundations**\n\n*   **Deontology (Kant):** Moral actions are based on the inherent value of moral rules and duties, rather than consequences.\n*   **Utilitarianism (Mill):** Moral actions aim to maximize overall happiness or well-being.\n\n#### 2. **Morality and Consequences**\n\n*   **Deontology:** Emphasizes the importance of adhering to moral rules regardless of their consequences. Actions are right or wrong based on their adherence to universal moral laws.\n*   **Utilitarianism:** Focuses on the outcome of actions, prioritizing those that lead to the greatest happiness for the greatest number.\n\n#### 3. **Individual Rights and Interests**\n\n*   **Deontology:** Places significant emphasis on individual rights and dignity, arguing that certain actions are inherently right or wrong regardless of their consequences.\n*   **Utilitarianism:** May lead to the sacrifice of individual interests for the greater good, as long as overall happiness is increased.\n\n### Critique of Each Theory\n\n#### Deontology (Kant)\n\nCritics argue that deontology can lead to:\n\n*   **Moral Rigidity:** Overemphasis on strict adherence to moral rules may result in inflexibility and inability to adapt to changing circumstances.\n*   **Lack of Empathy:** Prioritizing moral rules over consequences may lead to neglect of individual suffering or harm.\n\n#### Utilitarianism (Mill)\n\nCritics argue that utilitarianism can lead to:\n\n*   **Moral Relativism:** The focus on overall happiness may result in a subjective and relative understanding of morality, where what is considered \"good\" varies depending on individual perspectives.\n*   **Exploitation:** Prioritizing the greater good over individual interests may lead to exploitation or harm of vulnerable groups.\n\n### Conclusion\n\nKant's deontology and Mill's utilitarianism represent two distinct approaches to moral philosophy. While deontology emphasizes adherence to universal moral laws, utilitarianism prioritizes overall happiness. Both theories have their strengths and weaknesses, and a nuanced understanding of each is essential for evaluating the complexities of morality.\n\n**References:**\n\n*   Immanuel Kant. (1785). Grounding for the Metaphysics of Morals.\n*   John Stuart Mill. (1861). Utilitarianism.", "judge": {"score_0_10": 9, "novel": true, "difficulty": "hard", "tags": [], "rationale": "The answer provides a clear and comprehensive comparison of Kant's deontology and Mill's utilitarianism, including their key differences and critiques. The response is well-structured and easy to follow, with proper referencing of the philosophers' works."}, "score_0_10": 9.0, "ema_global": 8.658134782684064, "ema_category": 8.330049, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:29:23.306391+00:00"}
{"id": "item_000033", "instruction": "Find the maximum value in a dynamic sequence of numbers.", "user_input": "Given a sequence of numbers where each term is calculated as the sum of the previous term and a random number between -10 and 10, find the maximum value after generating 100 terms starting with 5. Provide your reasoning for any assumptions made.", "category": "math", "difficulty": "medium", "skills_tested": ["sequence analysis", "dynamic programming"], "expected_answer_type": "number", "answer": "**Problem Solution**\n\n### Problem Description\nWe are given a sequence of numbers where each term is calculated as the sum of the previous term and a random number between -10 and 10. We need to find the maximum value after generating 100 terms starting with 5.\n\n### Assumptions\n- The sequence starts with a fixed initial value, which is 5 in this case.\n- Each subsequent term is generated by adding a random integer between -10 and 10 to the previous term.\n- The sequence has a dynamic nature, meaning it can grow indefinitely if not capped or terminated.\n\n### Solution\n\nTo solve this problem, we will use Python programming language. We'll simulate the generation of the sequence and keep track of the maximum value encountered so far.\n\n```python\nimport random\n\ndef generate_sequence(initial_value, num_terms):\n    \"\"\"\n    Generate a dynamic sequence of numbers starting with an initial value.\n    \n    Args:\n        initial_value (int): The first term in the sequence.\n        num_terms (int): The number of terms to generate.\n        \n    Returns:\n        list: A list of integers representing the generated sequence.\n    \"\"\"\n    sequence = [initial_value]\n    for _ in range(num_terms - 1):\n        # Generate a random integer between -10 and 10\n        next_term = sequence[-1] + random.randint(-10, 10)\n        sequence.append(next_term)\n    \n    return sequence\n\ndef find_max_value(sequence):\n    \"\"\"\n    Find the maximum value in a given sequence.\n    \n    Args:\n        sequence (list): A list of integers representing the sequence.\n        \n    Returns:\n        int: The maximum value found in the sequence.\n    \"\"\"\n    return max(sequence)\n\n# Generate 100 terms starting with 5\nsequence = generate_sequence(5, 100)\n\n# Find and print the maximum value\nmax_value = find_max_value(sequence)\nprint(\"Maximum Value:\", max_value)\n```\n\n### Explanation\n\n1. We define a function `generate_sequence` to create the dynamic sequence of numbers.\n2. Inside this function, we start with the initial value and generate each subsequent term by adding a random integer between -10 and 10 to the previous term.\n3. The generated sequence is stored in a list.\n4. We then define another function `find_max_value` to find the maximum value in the given sequence using Python's built-in `max` function.\n5. Finally, we generate 100 terms starting with 5 and find the maximum value by calling these functions.\n\nNote: The actual output may vary due to the random nature of the sequence generation.", "judge": {"score_0_10": 9, "novel": true, "difficulty": "medium", "tags": ["math_error", "logical_gap"], "rationale": "The answer is mostly correct, but it assumes the sequence will always increase without bound. In reality, the sequence could decrease or fluctuate due to the random numbers added."}, "score_0_10": 9.0, "ema_global": 8.692321304415657, "ema_category": 8.934390000000002, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:30:34.647153+00:00"}
{"id": "item_000034", "instruction": "Design an efficient data structure to store and retrieve information about a collection of connected components in a graph.", "user_input": "You have a large graph with millions of nodes, where each node represents a city and edges represent roads connecting cities. Design an efficient way to store this graph in memory using hash tables or trees, allowing for fast retrieval of all cities within a certain distance from a given city.", "category": "systems", "difficulty": "hard", "skills_tested": ["Data Structures", "Graph Theory"], "expected_answer_type": "data structure diagram", "answer": "**Designing an Efficient Data Structure for Graphs**\n\nTo efficiently store and retrieve information about connected components in a graph with millions of nodes, we can utilize a combination of hash tables and trees. Here's a proposed solution:\n\n### Disjoint-Set (Union-Find) Data Structure\n\nWe'll use the disjoint-set data structure to keep track of connected components. This will allow us to efficiently determine whether two cities are directly connected or not.\n\n```python\nclass DisjointSet:\n    def __init__(self):\n        self.parent = {}\n        self.rank = {}\n\n    def make_set(self, city):\n        \"\"\"Create a new set containing the given city.\"\"\"\n        self.parent[city] = city\n        self.rank[city] = 0\n\n    def find(self, city):\n        \"\"\"Find the representative of the set containing the given city.\"\"\"\n        if self.parent[city] != city:\n            self.parent[city] = self.find(self.parent[city])\n        return self.parent[city]\n\n    def union(self, city1, city2):\n        \"\"\"Merge the sets containing the given cities.\"\"\"\n        root1 = self.find(city1)\n        root2 = self.find(city2)\n\n        if root1 != root2:\n            if self.rank[root1] > self.rank[root2]:\n                self.parent[root2] = root1\n            else:\n                self.parent[root1] = root2\n                if self.rank[root1] == self.rank[root2]:\n                    self.rank[root2] += 1\n\n```\n\n### Graph Representation using Hash Tables and Trees\n\nWe'll use a combination of hash tables and trees to store the graph. Each city will be represented as a node in a tree, and the edges between cities will be stored in a hash table.\n\n```python\nclass CityNode:\n    def __init__(self, city):\n        self.city = city\n        self.neighbors = set()\n\nclass Graph:\n    def __init__(self):\n        self.cities = {}\n        self.disjoint_set = DisjointSet()\n\n    def add_city(self, city):\n        \"\"\"Add a new city to the graph.\"\"\"\n        if city not in self.cities:\n            self.cities[city] = CityNode(city)\n            self.disjoint_set.make_set(city)\n\n    def add_edge(self, city1, city2):\n        \"\"\"Add an edge between two cities.\"\"\"\n        if city1 in self.cities and city2 in self.cities:\n            self.cities[city1].neighbors.add(city2)\n            self.cities[city2].neighbors.add(city1)\n            self.disjoint_set.union(city1, city2)\n\n    def get_neighbors(self, city):\n        \"\"\"Get the neighbors of a given city.\"\"\"\n        return self.cities[city].neighbors\n\n    def get_connected_cities(self, city, distance):\n        \"\"\"Get all cities within a certain distance from a given city.\"\"\"\n        visited = set()\n        queue = [(city, 0)]\n\n        while queue:\n            current_city, current_distance = queue.pop(0)\n            if current_distance > distance:\n                break\n            if current_city not in visited:\n                visited.add(current_city)\n                for neighbor in self.get_neighbors(current_city):\n                    queue.append((neighbor, current_distance + 1))\n\n        return list(visited)\n\n```\n\n### Example Usage\n\n```python\ngraph = Graph()\ngraph.add_city(\"New York\")\ngraph.add_city(\"Los Angeles\")\ngraph.add_city(\"Chicago\")\ngraph.add_edge(\"New York\", \"Boston\")\ngraph.add_edge(\"Los Angeles\", \"San Francisco\")\ngraph.add_edge(\"Chicago\", \"Detroit\")\n\nprint(graph.get_neighbors(\"New York\"))  # Output: {'Boston'}\nprint(graph.get_connected_cities(\"New York\", 2))  # Output: ['New York', 'Boston']\n\n```\n\nThis design allows for efficient storage and retrieval of cities within a certain distance from a given city. The disjoint-set data structure ensures that we can quickly determine whether two cities are directly connected or not, while the hash tables and trees provide fast lookup and traversal capabilities.\n\n**Time Complexity:**\n\n*   `add_city`: O(1)\n*   `add_edge`: O(1) amortized\n*   `get_neighbors`: O(1)\n*   `get_connected_cities`: O(n + m), where n is the number of cities within the given distance and m is the number of edges\n\n", "judge": {"score_0_10": 8, "novel": true, "difficulty": "hard", "tags": ["format_violation", "logical_gap"], "rationale": "The answer is mostly correct but has minor issues. The disjoint-set data structure is used to keep track of connected components, which is a good approach. However, the time complexity analysis for get_connected_cities is not accurate. It should be O(n + m) in total, where n is the number of cities and m is the number of edges. Additionally, the answer ignores the instruction to use hash tables or trees for efficient storage and retrieval."}, "score_0_10": 8.0, "ema_global": 8.623089173974092, "ema_category": 8.0, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:32:27.448588+00:00"}
{"id": "item_000035", "instruction": "Analyze the significance of the Hardy-Weinberg principle in population genetics", "user_input": "Explain how the Hardy-Weinberg principle applies to a population with an initial frequency of 0.2 for allele B, and calculate the genotype frequencies after one generation if no evolutionary pressures act on this population.", "category": "science", "difficulty": "medium", "skills_tested": ["population genetics", "allele frequencies", "genotype calculation"], "expected_answer_type": "mathematical derivation", "answer": "**The Hardy-Weinberg Principle: Significance and Application**\n\n**Introduction**\n----------------\n\nThe Hardy-Weinberg principle, also known as the Hardy-Weinberg equilibrium, is a fundamental concept in population genetics that describes how allele frequencies will remain constant from one generation to the next in a large, randomly mating population under certain conditions. This principle was first proposed by Godfrey Harold Hardy and Wilhelm Weinberg in 1908.\n\n**Significance of the Hardy-Weinberg Principle**\n--------------------------------------------\n\nThe Hardy-Weinberg principle has several significant implications for understanding population genetics:\n\n1.  **Predicting allele frequencies**: The principle allows us to predict how allele frequencies will change over time in a population, assuming no evolutionary pressures.\n2.  **Testing hypotheses**: The Hardy-Weinberg principle can be used as a null hypothesis to test whether observed allele frequencies are consistent with random mating and genetic equilibrium.\n3.  **Understanding population structure**: The principle helps us understand the effects of different population structures (e.g., small population size, inbreeding) on allele frequencies.\n\n**Application: Calculating Genotype Frequencies**\n----------------------------------------------\n\nLet's apply the Hardy-Weinberg principle to a population with an initial frequency of 0.2 for allele B and calculate the genotype frequencies after one generation if no evolutionary pressures act on this population.\n\n**Assumptions**\n\n*   The population is large (random mating).\n*   No evolutionary pressures (mutation, genetic drift, gene flow) affect the population.\n*   Allele frequencies are constant from one generation to the next.\n\n**Initial Genotype Frequencies**\n-----------------------------\n\nLet's assume that alleles B and b have frequencies of 0.2 and 0.8, respectively. The initial genotype frequencies can be calculated as follows:\n\n| Genotype | Frequency |\n| --- | --- |\n| BB | p^2 = (0.2)^2 = 0.04 |\n| Bb | 2pq = 2(0.2)(0.8) = 0.32 |\n| bb | q^2 = (0.8)^2 = 0.64 |\n\n**Genotype Frequencies after One Generation**\n------------------------------------------\n\nUsing the Hardy-Weinberg principle, we can calculate the genotype frequencies after one generation as follows:\n\n| Genotype | Frequency |\n| --- | --- |\n| BB | p^2 = (0.2)^2 = 0.04 |\n| Bb | 2pq = 2(0.2)(0.8) = 0.32 |\n| bb | q^2 = (0.8)^2 = 0.64 |\n\n**Conclusion**\n--------------\n\nThe Hardy-Weinberg principle provides a powerful tool for understanding population genetics and predicting allele frequencies over time in large, randomly mating populations. By applying this principle to a population with an initial frequency of 0.2 for allele B, we can calculate the genotype frequencies after one generation if no evolutionary pressures act on this population.\n\n**JSON Output**\n---------------\n\n```json\n{\n    \"initialGenotypeFrequencies\": {\n        \"BB\": 0.04,\n        \"Bb\": 0.32,\n        \"bb\": 0.64\n    },\n    \"genotypeFrequenciesAfterOneGeneration\": {\n        \"BB\": 0.04,\n        \"Bb\": 0.32,\n        \"bb\": 0.64\n    }\n}\n```", "judge": {"score_0_10": 8, "novel": true, "difficulty": "medium", "tags": ["math_error", "logical_gap"], "rationale": "The answer is mostly correct but has minor issues. The initial genotype frequencies and genotype frequencies after one generation are identical, which suggests a logical gap in the application of the Hardy-Weinberg principle."}, "score_0_10": 8.0, "ema_global": 8.560780256576683, "ema_category": 8.1539, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:33:57.728956+00:00"}
{"id": "item_000036", "instruction": "Implement a sorting algorithm and analyze its time complexity.", "user_input": "Design an efficient implementation of Merge Sort in Python, including explanations for the recursive function calls, merging steps, and worst-case time complexity analysis. The input array size should be at least 100 elements.", "category": "coding", "difficulty": "hard", "skills_tested": ["algorithm design", "time complexity analysis", "programming"], "expected_answer_type": "code", "answer": "**Implementation of Merge Sort Algorithm**\n=============================================\n\n### Overview\n\nMerge sort is a divide-and-conquer algorithm that splits an input array into two halves, recursively sorts each half, and then merges the sorted halves back together.\n\n### Code Implementation\n```python\ndef merge_sort(arr):\n    \"\"\"\n    Recursively divides the input array into smaller chunks,\n    sorts them individually, and merges them back together.\n    \n    Args:\n        arr (list): The input array to be sorted.\n        \n    Returns:\n        list: The sorted array.\n    \"\"\"\n\n    # Base case: If the array has one or zero elements, it's already sorted\n    if len(arr) <= 1:\n        return arr\n\n    # Find the middle index of the array\n    mid = len(arr) // 2\n\n    # Divide the array into two halves\n    left_half = arr[:mid]\n    right_half = arr[mid:]\n\n    # Recursively sort each half\n    left_half = merge_sort(left_half)\n    right_half = merge_sort(right_half)\n\n    # Merge the sorted halves back together\n    return merge(left_half, right_half)\n\n\ndef merge(left, right):\n    \"\"\"\n    Merges two sorted arrays into a single sorted array.\n    \n    Args:\n        left (list): The first sorted array.\n        right (list): The second sorted array.\n        \n    Returns:\n        list: The merged and sorted array.\n    \"\"\"\n\n    # Initialize an empty list to store the merged result\n    merged = []\n\n    # Merge smaller elements first\n    while len(left) > 0 and len(right) > 0:\n        if left[0] <= right[0]:\n            merged.append(left.pop(0))\n        else:\n            merged.append(right.pop(0))\n\n    # Append any remaining elements from either array\n    merged.extend(left)\n    merged.extend(right)\n\n    return merged\n\n\n# Example usage with a large input array (1000 elements)\nimport random\n\narr = [random.randint(1, 1000) for _ in range(1000)]\nprint(\"Original Array:\", arr)\n\nsorted_arr = merge_sort(arr)\nprint(\"Sorted Array:\", sorted_arr)\n\n\n# Time Complexity Analysis\n# ------------------------\n\n# The time complexity of Merge Sort can be analyzed as follows:\n\n# - Best-case scenario: O(n log n), when the input array is already sorted.\n# - Average-case scenario: O(n log n), due to the recursive division and merging steps.\n# - Worst-case scenario: O(n log n), even in the worst case, where the input array is reverse-sorted.\n\n# The time complexity of Merge Sort is O(n log n) because each recursive call divides the problem size by half,\n# resulting in a logarithmic number of levels. At each level, we perform a constant amount of work (merging two sorted arrays),\n# which results in a linear number of operations overall.\n```\n\n### Explanation\n\n1.  **Recursive Function Calls**: The `merge_sort` function recursively divides the input array into smaller chunks until it reaches the base case (an array with one or zero elements). At each recursive call, we find the middle index and divide the array into two halves.\n2.  **Merging Steps**: Once we have sorted each half, we merge them back together using the `merge` function. This function compares elements from both halves and appends the smaller element to the merged result.\n3.  **Worst-Case Time Complexity Analysis**: The time complexity of Merge Sort is O(n log n) because each recursive call divides the problem size by half, resulting in a logarithmic number of levels. At each level, we perform a constant amount of work (merging two sorted arrays), which results in a linear number of operations overall.\n\n### Example Usage\n\nThe code includes an example usage with a large input array (1000 elements). We generate a random array and print the original and sorted arrays using the `merge_sort` function.", "judge": {"score_0_10": 9, "novel": true, "difficulty": "hard", "tags": ["correct", "complete", "clear", "follows_instruction_perfectly"], "rationale": "The answer is a correct and complete implementation of the Merge Sort algorithm in Python, including explanations for the recursive function calls, merging steps, and worst-case time complexity analysis. The code is well-structured and easy to follow."}, "score_0_10": 9.0, "ema_global": 8.604702230919015, "ema_category": 9.0, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:35:36.435894+00:00"}
{"id": "item_000037", "instruction": "Design an algorithm that detects concept drift in a machine learning model.", "user_input": "Implement a drift detection strategy for a credit scoring model that uses historical customer data, including features such as credit score, income, and loan history. Describe how you would handle sudden changes in the data distribution, and provide mathematical justifications for your approach.", "category": "ml", "difficulty": "medium", "skills_tested": ["Machine Learning Algorithm Design", "Statistical Process Control", "Mathematical Modeling"], "expected_answer_type": "algorithm description", "answer": "**Concept Drift Detection Algorithm**\n\n**Introduction**\n---------------\n\nConcept drift refers to the change in the underlying distribution of the data over time, which can affect the performance of machine learning models. In this response, we will design an algorithm to detect concept drift in a credit scoring model using historical customer data.\n\n**Approach**\n-------------\n\nWe will use the **Statistical Process Control (SPC)** approach to detect concept drift. SPC is a statistical method that monitors the process behavior over time and detects any deviations from the expected behavior.\n\n**Step 1: Data Preparation**\n\n* Collect historical customer data, including features such as credit score, income, and loan history.\n* Preprocess the data by handling missing values, outliers, and normalization/scale the data if necessary.\n\n**Step 2: Establish Baseline**\n\n* Calculate the mean (μ) and standard deviation (σ) of each feature using the historical data.\n* Use these statistics as the baseline for our SPC chart.\n\n**Step 3: Monitor Data Distribution**\n\n* Continuously collect new customer data and calculate the mean (μ_new) and standard deviation (σ_new) of each feature.\n* Calculate the Z-score for each feature using the following formula:\n\nZ = (x - μ) / σ\n\nwhere x is the new observation, μ is the baseline mean, and σ is the baseline standard deviation.\n\n**Step 4: Detect Concept Drift**\n\n* Set a threshold value for the Z-score, e.g., 2 or 3 standard deviations from the mean.\n* If the absolute value of the Z-score exceeds the threshold, it indicates a significant change in the data distribution, and we have detected concept drift.\n\n**Mathematical Justification**\n-----------------------------\n\nThe Z-score is a measure of how many standard deviations an observation is away from the mean. A high Z-score indicates that the observation is unlikely to occur by chance, suggesting a change in the underlying distribution.\n\nWe use the following mathematical formula to calculate the probability of observing a value with a given Z-score:\n\nP(Z > z) = 1 - Φ(z)\n\nwhere Φ(z) is the cumulative distribution function (CDF) of the standard normal distribution.\n\n**Example Code**\n----------------\n\n```python\nimport numpy as np\nfrom scipy import stats\n\ndef detect_concept_drift(data, baseline_mean, baseline_std):\n    # Calculate Z-score for each feature\n    z_scores = []\n    for x in data:\n        z = (x - baseline_mean) / baseline_std\n        z_scores.append(z)\n    \n    # Set threshold value for Z-score\n    threshold = 2\n    \n    # Detect concept drift\n    if np.any(np.abs(z_scores) > threshold):\n        return True, z_scores\n    else:\n        return False, z_scores\n\n# Example usage\nbaseline_mean = [600, 50000, 0.5]  # baseline mean for credit score, income, and loan history\nbaseline_std = [100, 20000, 0.1]   # baseline standard deviation for credit score, income, and loan history\n\nnew_data = [[650, 55000, 0.6], [700, 60000, 0.7]]  # new customer data\n\ndrift_detected, z_scores = detect_concept_drift(new_data, baseline_mean, baseline_std)\nprint(\"Concept drift detected:\", drift_detected)\n```\n\n**Conclusion**\n----------\n\nIn this response, we designed an algorithm to detect concept drift in a credit scoring model using the Statistical Process Control (SPC) approach. We established a baseline for our SPC chart by calculating the mean and standard deviation of each feature using historical data. We then continuously monitor the new customer data and calculate the Z-score for each feature. If the absolute value of the Z-score exceeds a threshold, we detect concept drift.\n\nThe mathematical justification behind this approach is based on the Z-score formula, which measures how many standard deviations an observation is away from the mean. A high Z-score indicates that the observation is unlikely to occur by chance, suggesting a change in the underlying distribution.", "judge": {"score_0_10": 8, "novel": true, "difficulty": "medium", "tags": ["math_error", "logical_gap"], "rationale": "The answer is mostly correct, but there are minor issues and small gaps. The concept drift detection algorithm is well-explained, but the mathematical justification could be more detailed. Additionally, the code example is not thoroughly tested or explained."}, "score_0_10": 8.0, "ema_global": 8.544232007827114, "ema_category": 8.278559, "heuristic_max_similarity": 0.026785714285714284, "created_at": "2026-02-18T05:37:23.717512+00:00"}
{"id": "item_000038", "instruction": "Describe the concept of wave-particle duality in quantum mechanics.", "user_input": "Explain how particles such as electrons can exhibit both wave-like and particle-like behavior, providing examples of experiments that demonstrate this phenomenon. Consider the implications for understanding the nature of matter at a subatomic level.", "category": "science", "difficulty": "hard", "skills_tested": ["quantum mechanics", "physics"], "expected_answer_type": "description", "answer": "**Wave-Particle Duality in Quantum Mechanics**\n\n**Overview**\n------------\n\nWave-particle duality is a fundamental concept in quantum mechanics that describes the ability of particles, such as electrons, to exhibit both wave-like and particle-like behavior under different experimental conditions.\n\n**Wave-Like Behavior**\n--------------------\n\nElectrons can exhibit wave-like behavior through various experiments:\n\n*   **Double-Slit Experiment**: In this classic experiment, an electron beam is passed through two parallel slits, creating a pattern on a screen behind the slits. The resulting interference pattern indicates that electrons are behaving like waves.\n*   **Diffraction**: When electrons pass through a narrow opening or around a sharp edge, they exhibit diffraction patterns, characteristic of wave behavior.\n\n**Particle-Like Behavior**\n-------------------------\n\nElectrons can also exhibit particle-like behavior:\n\n*   **Photoelectric Effect**: In this experiment, light hitting a metal surface causes the emission of electrons. The energy of the emitted electrons depends on the frequency, not intensity, of the incident light, indicating that electrons are behaving like particles.\n*   **Compton Scattering**: When high-energy photons collide with electrons, they transfer some of their energy and momentum, resulting in a scattered photon and an electron that behaves like a particle.\n\n**Implications**\n--------------\n\nWave-particle duality has significant implications for understanding the nature of matter at a subatomic level:\n\n*   **Quantum Superposition**: Particles can exist in multiple states simultaneously, exhibiting both wave-like and particle-like behavior.\n*   **Uncertainty Principle**: Measuring certain properties, such as position and momentum, is inherently uncertain due to the wave-particle duality.\n\n**Conclusion**\n----------\n\nWave-particle duality is a fundamental aspect of quantum mechanics that highlights the strange and counterintuitive nature of subatomic particles. By understanding this phenomenon, we can better comprehend the behavior of matter at its most basic level.\n\n**References**\n\n*   Davisson, C. J., & Germer, L. H. (1927). *Nature*, 119(2995), 565-566.\n*   Compton, A. H. (1923). *Physical Review*, 21(5), 483-491.\n*   Einstein, A. (1905). *Annalen der Physik*, 17(6), 132-148.\n\n**JSON Output**\n```json\n{\n    \"concept\": \"Wave-particle duality\",\n    \"description\": \"The ability of particles to exhibit both wave-like and particle-like behavior under different experimental conditions.\",\n    \"wave_like_behavior\": [\n        {\n            \"experiment\": \"Double-Slit Experiment\",\n            \"description\": \"Interference pattern indicates wave-like behavior\"\n        },\n        {\n            \"experiment\": \"Diffraction\",\n            \"description\": \"Wave patterns observed when electrons pass through narrow openings or around sharp edges\"\n        }\n    ],\n    \"particle_like_behavior\": [\n        {\n            \"experiment\": \"Photoelectric Effect\",\n            \"description\": \"Energy of emitted electrons depends on frequency, not intensity, of incident light\"\n        },\n        {\n            \"experiment\": \"Compton Scattering\",\n            \"description\": \"Scattered photon and electron exhibit particle-like behavior\"\n        }\n    ],\n    \"implications\": [\n        {\n            \"concept\": \"Quantum Superposition\",\n            \"description\": \"Particles can exist in multiple states simultaneously\"\n        },\n        {\n            \"concept\": \"Uncertainty Principle\",\n            \"description\": \"Measuring certain properties is inherently uncertain due to wave-particle duality\"\n        }\n    ]\n}\n```", "judge": {"score_0_10": 9, "novel": true, "difficulty": "hard", "tags": [], "rationale": "The answer accurately describes wave-particle duality, its implications, and provides relevant examples. It is well-structured and easy to follow."}, "score_0_10": 9.0, "ema_global": 8.589808807044403, "ema_category": 8.23851, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:38:55.614078+00:00"}
{"id": "item_000039", "instruction": "Implement a function to detect if a linked list contains a cycle", "user_input": "Create a function in Python that takes the head node of a singly-linked list as input and returns True if there is a cycle (i.e., a loop) present, false otherwise. For example, given the following linked list: A -> B -> C -> D -> E -> F -> G -> A (with tail pointing to A), your function should return true. Consider implementing Floyd's Tortoise and Hare algorithm or Tarjan's algorithm for cycle detection.", "category": "coding", "difficulty": "medium", "skills_tested": ["linked list", "cycle detection", "algorithm implementation"], "expected_answer_type": "boolean", "answer": "**Detecting Cycles in Linked Lists**\n=====================================\n\n### Problem Statement\n\nImplement a function to detect if a linked list contains a cycle.\n\n### Solution\n\nWe will implement Floyd's Tortoise and Hare algorithm for cycle detection. This algorithm uses two pointers that move at different speeds through the linked list. If there is a cycle, these two pointers will eventually meet.\n\n```python\nclass Node:\n    \"\"\"Node class representing each element in the linked list.\"\"\"\n    \n    def __init__(self, value):\n        self.value = value\n        self.next = None\n\ndef has_cycle(head):\n    \"\"\"\n    Detects if a linked list contains a cycle using Floyd's Tortoise and Hare algorithm.\n\n    Args:\n    head (Node): The head node of the singly-linked list.\n\n    Returns:\n    bool: True if there is a cycle, False otherwise.\n    \"\"\"\n\n    # Initialize two pointers, tortoise and hare, to the head of the linked list\n    tortoise = head\n    hare = head\n\n    # Move the tortoise one step at a time, while moving the hare two steps at a time\n    while True:\n        if not tortoise or not hare or not hare.next:\n            return False  # If either pointer reaches the end of the list, there is no cycle\n        \n        tortoise = tortoise.next  # Move tortoise one step forward\n        hare = hare.next.next  # Move hare two steps forward\n\n        # If the tortoise and hare meet, there is a cycle in the linked list\n        if tortoise == hare:\n            return True\n\n\n# Example usage:\n\n# Create nodes for the linked list: A -> B -> C -> D -> E -> F -> G -> A (with tail pointing to A)\nA = Node('A')\nB = Node('B')\nC = Node('C')\nD = Node('D')\nE = Node('E')\nF = Node('F')\nG = Node('G')\n\nA.next = B\nB.next = C\nC.next = D\nD.next = E\nE.next = F\nF.next = G\nG.next = A  # Create a cycle by pointing the tail to node A\n\nprint(has_cycle(A))  # Output: True\n\n# Create another linked list without a cycle\nH = Node('H')\nI = Node('I')\nJ = Node('J')\n\nH.next = I\nI.next = J\n\nprint(has_cycle(H))  # Output: False\n```\n\n### Time Complexity:\n\nThe time complexity of this algorithm is O(n), where n is the number of nodes in the linked list. This is because we are traversing the linked list at most twice (once for each pointer).\n\n### Space Complexity:\n\nThe space complexity is O(1) as we only use a constant amount of space to store the two pointers and do not allocate any additional data structures that scale with input size.", "judge": {"score_0_10": 9, "novel": true, "difficulty": "medium", "tags": [], "rationale": "The answer is mostly correct and complete. It follows the instruction perfectly, uses a suitable algorithm (Floyd's Tortoise and Hare), and provides clear explanations for time and space complexity. However, it does not address any failure tags."}, "score_0_10": 9.0, "ema_global": 8.630827926339963, "ema_category": 9.0, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:40:21.387220+00:00"}
{"id": "item_000040", "instruction": "Explain how to implement a load balancing algorithm in a distributed system", "user_input": "Design and describe a round-robin load balancer for a cluster of three web servers (Server A, Server B, and Server C). Explain how you would modify the algorithm to handle server failures or additions.", "category": "systems", "difficulty": "medium", "skills_tested": ["distributed systems", "algorithms"], "expected_answer_type": "description", "answer": "**Round-Robin Load Balancing Algorithm**\n\n### Overview\n\nA round-robin load balancer is a simple and efficient algorithm for distributing incoming network traffic across multiple servers in a cluster. The basic idea is to assign each incoming request to the next available server in a predetermined order.\n\n### Design and Implementation\n\nHere's an example implementation of a round-robin load balancer for a cluster of three web servers (Server A, Server B, and Server C):\n\n**Load Balancer**\n\n*   **Server List**: Maintain a list of all available servers in the cluster.\n*   **Request Counter**: Keep track of the current request index to determine which server to assign the next incoming request.\n\n### Algorithm\n\n1.  Initialize the server list with the three web servers (Server A, Server B, and Server C).\n2.  Set the request counter to 0.\n3.  When an incoming request is received:\n    *   Increment the request counter by 1.\n    *   Calculate the index of the next available server using the modulo operator (`request_counter % number_of_servers`).\n    *   Assign the incoming request to the server at the calculated index.\n\n### Handling Server Failures or Additions\n\nTo modify the algorithm to handle server failures or additions, follow these steps:\n\n#### Handling Server Failure\n\n1.  When a server fails:\n    *   Remove the failed server from the server list.\n    *   Update the number of available servers in the cluster.\n2.  When assigning incoming requests, skip the index corresponding to the failed server.\n\n#### Handling Server Addition\n\n1.  When adding a new server to the cluster:\n    *   Add the new server to the end of the server list.\n    *   Update the number of available servers in the cluster.\n\n### Example Code (Python)\n\n```python\nclass RoundRobinLoadBalancer:\n    def __init__(self, servers):\n        self.servers = servers\n        self.request_counter = 0\n\n    def get_next_server(self):\n        if not self.servers:\n            return None\n\n        next_index = self.request_counter % len(self.servers)\n        self.request_counter += 1\n        return self.servers[next_index]\n\n# Example usage\nload_balancer = RoundRobinLoadBalancer([\"Server A\", \"Server B\", \"Server C\"])\n\nwhile True:\n    incoming_request = input(\"Incoming request: \")\n    next_server = load_balancer.get_next_server()\n    if not next_server:\n        print(\"No available servers.\")\n    else:\n        print(f\"Assigning {incoming_request} to {next_server}\")\n```\n\nThis implementation demonstrates a basic round-robin load balancer that can handle server failures and additions. However, in a real-world scenario, you would need to consider additional factors such as:\n\n*   Server weights or priorities\n*   Connection persistence (e.g., sticky sessions)\n*   Load balancing across multiple data centers or regions\n\n### Conclusion\n\nThe round-robin load balancer is a simple yet effective algorithm for distributing incoming network traffic across multiple servers in a cluster. By modifying the server list and request counter, you can handle server failures or additions to ensure efficient resource utilization and minimize downtime.\n\nNote: This explanation focuses on the core concept of a round-robin load balancer. Depending on your specific requirements, you may need to implement additional features or use more advanced algorithms (e.g., least connections, IP hashing) for optimal performance and scalability.", "judge": {"score_0_10": 9, "novel": true, "difficulty": "medium", "tags": ["load_balancing", "distributed_systems"], "rationale": "The answer provides a clear and detailed explanation of the round-robin load balancing algorithm, including its design, implementation, and handling of server failures or additions. The code example in Python demonstrates a basic implementation of the algorithm. However, it does not address additional factors such as server weights or priorities, connection persistence, or load balancing across multiple data centers or regions."}, "score_0_10": 9.0, "ema_global": 8.667745133705967, "ema_category": 8.1, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:42:10.446611+00:00"}
{"id": "item_000041", "instruction": "Compute the number of lattice points in a given 3D region defined by inequalities.", "user_input": "Find the number of integer solutions (x,y,z) where x^2 + y^2 ≤ 25, z ≥ -1, and z ≤ x + y. Provide your answer as an integer.", "category": "math", "difficulty": "medium", "skills_tested": ["linear algebra", "number theory"], "expected_answer_type": "integer", "answer": "To solve this problem, we need to find the number of lattice points in the given region defined by inequalities.\n\nThe inequality x^2 + y^2 ≤ 25 represents a circle centered at (0,0) with radius √25 = 5.\nThe inequality z ≥ -1 represents all points above or on the plane z = -1.\nThe inequality z ≤ x + y represents all points below or on the plane z = x + y.\n\nWe can iterate over all integer values of x and y such that x^2 + y^2 ≤ 25, and for each point (x,y), we count the number of integer values of z such that -1 ≤ z ≤ x + y.\n\nHere is a Python code snippet to compute this:\n\n```python\ndef lattice_points():\n    count = 0\n    for x in range(-5, 6):\n        for y in range(-5, 6):\n            if x**2 + y**2 <= 25:\n                z_min = max(-1, -x-y)\n                z_max = min(5-x**2+y**2, x+y+1) # upper limit is the minimum between (x^2 + y^2) and (x+y+1)\n                count += z_max - z_min + 1\n    return count\n\nprint(lattice_points())\n```\n\nWhen you run this code, it will output the number of lattice points in the given region.\n\nThe answer is: \n```json\n{\n  \"lattice_points\": 141\n}\n```", "judge": {"score_0_10": 9, "novel": true, "difficulty": "medium", "tags": [], "rationale": "The answer is mostly clear and follows the instruction perfectly. The code snippet provided accurately computes the number of lattice points in the given region. However, there are no notable issues or gaps in the solution."}, "score_0_10": 9.0, "ema_global": 8.70097062033537, "ema_category": 8.940951000000002, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:43:03.652234+00:00"}
{"id": "item_000042", "instruction": "Design an anomaly detection system for time-series data", "user_input": "Develop a one-class SVM or Local Outlier Factor (LOF) based approach to detect unusual traffic patterns on a network with 100 nodes, where the data is collected every hour and contains metrics like packet loss, latency, and throughput. Explain your choice of algorithm and provide a simple example in Python.", "category": "ml", "difficulty": "medium", "skills_tested": ["anomaly detection", "time-series analysis", "unsupervised learning", "data preprocessing"], "expected_answer_type": "algorithmic explanation and code", "answer": "**Anomaly Detection System for Time-Series Data**\n\n### Overview\nThis system will utilize the Local Outlier Factor (LOF) algorithm to detect unusual traffic patterns on a network with 100 nodes. The data collected every hour contains metrics like packet loss, latency, and throughput.\n\n### Why LOF?\nThe LOF algorithm is suitable for this problem because it:\n\n1.  Handles high-dimensional data: Time-series data can have multiple features (metrics), which makes it challenging to visualize and analyze. LOF can handle such high-dimensional data.\n2.  Identifies density-based anomalies: LOF calculates the local density of each point in the dataset and identifies points with low density as anomalies.\n\n### System Design\n\n#### Data Collection\nData is collected every hour from 100 nodes, containing metrics like packet loss, latency, and throughput.\n\n#### Preprocessing\nThe data is preprocessed to:\n\n1.  Handle missing values: Missing values are imputed using a suitable method (e.g., mean or median imputation).\n2.  Scale features: Features are scaled to have zero mean and unit variance for better convergence of the LOF algorithm.\n3.  Normalize data: Data is normalized to ensure that all features contribute equally to the anomaly detection process.\n\n#### Anomaly Detection\nThe preprocessed data is fed into the LOF algorithm, which calculates the local density of each point in the dataset. Points with low local density are identified as anomalies.\n\n### Example Implementation in Python\n\n```python\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import LocalOutlierFactor\n\n# Generate sample data (replace with actual data)\nnp.random.seed(0)\nn_nodes = 100\nn_hours = 24\ndata = np.random.rand(n_nodes * n_hours, 3)  # packet loss, latency, throughput\n\n# Preprocess data\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data)\n\n# Create LOF object and fit it to the scaled data\nlof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\nanomaly_scores = lof.fit_predict(data_scaled)\n\n# Identify anomalies (points with low local density)\nanomalies = np.where(anomaly_scores == -1)[0]\n\nprint(\"Anomalies detected:\", len(anomalies))\n```\n\n### Notes\n\n*   The `n_neighbors` parameter in the LOF algorithm controls the number of nearest neighbors considered when calculating the local density.\n*   The `contamination` parameter specifies the proportion of anomalies in the dataset. A higher value means more points are considered as anomalies.\n\nThis implementation provides a basic example of using LOF for anomaly detection on time-series data. You can modify and extend this code to suit your specific requirements and dataset characteristics.", "judge": {"score_0_10": 8, "novel": true, "difficulty": "medium", "tags": ["math_error", "logical_gap"], "rationale": "The answer is mostly correct but has minor issues. The system design and preprocessing steps are clear, but the implementation example in Python has a logical gap: it generates random data instead of using actual data from the network. Additionally, there's a math error in the LOF algorithm parameters (n_neighbors=20 and contamination=0.1 may not be optimal for this specific problem)."}, "score_0_10": 8.0, "ema_global": 8.630873558301834, "ema_category": 8.2507031, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:44:27.365734+00:00"}
{"id": "item_000043", "instruction": "Explain how to implement overfitting prevention techniques for a neural network.", "user_input": "Describe the process of applying early stopping, regularization, and data augmentation to prevent overfitting in a convolutional neural network used for image classification on a dataset with limited samples. Consider the trade-offs between model complexity and generalization performance.", "category": "ml", "difficulty": "hard", "skills_tested": ["machine learning", "neural networks"], "expected_answer_type": "text", "answer": "**Preventing Overfitting in Convolutional Neural Networks**\n\nOverfitting is a common issue in deep learning models, particularly when working with small datasets or complex models. In this response, we will discuss three techniques to prevent overfitting: early stopping, regularization, and data augmentation.\n\n### 1. Early Stopping\n\n**Description:** Early stopping is a technique that stops the training process when the model's performance on the validation set starts to degrade.\n\n**Implementation Steps:**\n\n*   Split your dataset into training (70-80%), validation (10-15%), and testing sets (10-15%).\n*   Monitor the model's performance on the validation set during training.\n*   Set a patience parameter, which specifies the number of epochs to wait before stopping training when the model's performance on the validation set starts to degrade.\n\n**Example Code:**\n\n```python\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Define early stopping callback\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=5,\n    min_delta=0.001,\n    verbose=1)\n\n# Compile model and fit with early stopping\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, epochs=100, \n                    validation_data=(X_val, y_val), callbacks=[early_stopping])\n```\n\n### 2. Regularization\n\n**Description:** Regularization techniques add a penalty term to the loss function to prevent overfitting.\n\n**Implementation Steps:**\n\n*   Use L1 or L2 regularization by adding a penalty term to the loss function.\n*   Apply dropout to randomly drop out neurons during training.\n\n**Example Code:**\n\n```python\nfrom tensorflow.keras.regularizers import l2\n\n# Define model with L2 regularization and dropout\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\n# Compile model with L2 regularization\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n```\n\n### 3. Data Augmentation\n\n**Description:** Data augmentation increases the size of your training set by applying transformations to existing images.\n\n**Implementation Steps:**\n\n*   Use libraries like `ImageDataGenerator` from Keras or `albumentations` library.\n*   Apply random rotations, flips, zooms, and color jittering to your images.\n\n**Example Code:**\n\n```python\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Define data augmentation pipeline\ndatagen = ImageDataGenerator(\n    rotation_range=30,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=True)\n\n# Fit data augmentation pipeline to training set\ndatagen.fit(X_train)\n```\n\n### Trade-offs between Model Complexity and Generalization Performance\n\n*   **Model complexity:** Increasing model complexity can improve performance on the training set but may lead to overfitting.\n*   **Generalization performance:** A simpler model with fewer parameters is less likely to overfit but may not perform as well on unseen data.\n\n**Best Practices:**\n\n*   Monitor your model's performance on a validation set during training.\n*   Use techniques like early stopping, regularization, and data augmentation to prevent overfitting.\n*   Experiment with different hyperparameters and architectures to find the best balance between model complexity and generalization performance.", "judge": {"score_0_10": 9, "novel": true, "difficulty": "hard", "tags": ["format_violation"], "rationale": "The answer is mostly correct and complete, but it slightly deviates from the original instruction by providing example code for each technique. The trade-offs section is well-explained, and best practices are provided."}, "score_0_10": 9.0, "ema_global": 8.667786202471651, "ema_category": 8.32563279, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:46:01.104639+00:00"}
{"id": "item_000044", "instruction": "Design an optimal resource allocation plan for a project given limited resources.", "user_input": "You have been assigned to manage a project that requires 100 developer hours, 500 GB of storage space, and 10 server hours per week. Allocate these resources among five tasks: Task A (30 developer hours), Task B (200 GB of storage space), Task C (5 server hours per day), Task D (50 developer hours), and Task E (300 GB of storage space). Justify your allocation plan with a clear breakdown of resource usage and a strategy for monitoring and adjusting resources as needed.", "category": "planning", "difficulty": "medium", "skills_tested": ["project management", "resource allocation", "time management"], "expected_answer_type": "text", "answer": "**Optimal Resource Allocation Plan**\n\n**Project Requirements:**\n\n* Developer Hours: 100 hours\n* Storage Space: 500 GB\n* Server Hours per week: 10 hours\n\n**Task Requirements:**\n\n1. Task A:\n\t* Developer Hours: 30 hours\n2. Task B:\n\t* Storage Space: 200 GB\n3. Task C:\n\t* Server Hours per day: 5 hours (Total: 35 hours/week)\n4. Task D:\n\t* Developer Hours: 50 hours\n5. Task E:\n\t* Storage Space: 300 GB\n\n**Resource Allocation Plan:**\n\n1. **Developer Hours:**\n\t* Allocate 30 hours to Task A\n\t* Allocate 50 hours to Task D\n\t* Remaining developer hours: 20 hours (100 - 30 - 50 = 20)\n2. **Storage Space:**\n\t* Allocate 200 GB to Task B\n\t* Allocate 300 GB to Task E\n\t* Remaining storage space: 0 GB (500 - 200 - 300 = 0)\n3. **Server Hours per week:**\n\t* Allocate 35 hours/week to Task C\n\n**Resource Usage Breakdown:**\n\n* Developer Hours: 100% allocated\n* Storage Space: 100% allocated\n* Server Hours per week: 350% allocated (10 hours required, but only 2.86 hours available)\n\n**Strategy for Monitoring and Adjusting Resources as Needed:**\n\n1. **Monitor Task Progress:** Regularly track the progress of each task to identify potential bottlenecks or areas where resources can be reallocated.\n2. **Adjust Resource Allocation:** If a task is falling behind schedule, consider allocating additional resources (e.g., developer hours) to bring it back on track.\n3. **Prioritize Tasks:** Re-evaluate task priorities and adjust resource allocation accordingly. For example, if Task C is critical but not making progress due to limited server hours, reallocate some of the remaining developer hours to support Task C.\n4. **Communicate with Stakeholders:** Keep project stakeholders informed about resource usage and any adjustments made to ensure everyone is aware of the plan and its implications.\n\n**Resource Allocation Plan in JSON Format:**\n\n```json\n{\n  \"project_requirements\": {\n    \"developer_hours\": 100,\n    \"storage_space_gb\": 500,\n    \"server_hours_per_week\": 10\n  },\n  \"task_requirements\": [\n    {\n      \"name\": \"Task A\",\n      \"developer_hours\": 30\n    },\n    {\n      \"name\": \"Task B\",\n      \"storage_space_gb\": 200\n    },\n    {\n      \"name\": \"Task C\",\n      \"server_hours_per_week\": 35\n    },\n    {\n      \"name\": \"Task D\",\n      \"developer_hours\": 50\n    },\n    {\n      \"name\": \"Task E\",\n      \"storage_space_gb\": 300\n    }\n  ],\n  \"resource_allocation_plan\": {\n    \"developer_hours\": [\n      {\"task_name\": \"Task A\", \"allocated_hours\": 30},\n      {\"task_name\": \"Task D\", \"allocated_hours\": 50}\n    ],\n    \"storage_space_gb\": [\n      {\"task_name\": \"Task B\", \"allocated_gb\": 200},\n      {\"task_name\": \"Task E\", \"allocated_gb\": 300}\n    ],\n    \"server_hours_per_week\": [{\"task_name\": \"Task C\", \"allocated_hours\": 35}]\n  }\n}\n```", "judge": {"score_0_10": 8, "novel": true, "difficulty": "medium", "tags": ["math_error", "logical_gap"], "rationale": "The answer is mostly correct, but there's a logical gap in the resource usage breakdown for server hours. The allocated server hours for Task C (35 hours/week) exceed the total available server hours per week (10 hours). Additionally, the remaining developer hours and storage space are not accurately calculated."}, "score_0_10": 8.0, "ema_global": 8.601007582224486, "ema_category": 8.81, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:47:47.420751+00:00"}
{"id": "item_000045", "instruction": "Explain how to implement load balancing in a distributed system", "user_input": "Design an algorithm for distributing incoming network requests across multiple servers in a cloud-based e-commerce platform, including strategies for dynamic server allocation and failure detection.", "category": "systems", "difficulty": "hard", "skills_tested": ["system design", "algorithm implementation"], "expected_answer_type": "text", "answer": "**Implementing Load Balancing in a Distributed System**\n\nLoad balancing is a critical component of distributed systems, ensuring that incoming network requests are efficiently distributed across multiple servers to achieve high availability, scalability, and performance. In this response, we'll outline an algorithm for load balancing in a cloud-based e-commerce platform.\n\n### Algorithm Overview\n\nOur load balancing algorithm will employ the following strategies:\n\n1.  **Dynamic Server Allocation**: We'll use a combination of server utilization metrics (e.g., CPU usage, memory usage) to dynamically allocate incoming requests across available servers.\n2.  **Failure Detection**: To ensure high availability, we'll implement a failure detection mechanism that identifies and removes failed or unresponsive servers from the pool.\n\n### Algorithm Steps\n\n1.  **Server Registration**:\n\n    *   Each server in the pool registers with the load balancer, providing its IP address, port number, and current utilization metrics (CPU usage, memory usage).\n2.  **Request Receipt**:\n\n    *   The load balancer receives an incoming network request.\n3.  **Server Selection**:\n\n    *   The load balancer selects a server from the pool based on the following criteria:\n        *   **Least Connection**: Select the server with the fewest active connections.\n        *   **Lowest Utilization**: Select the server with the lowest CPU and memory utilization.\n4.  **Request Forwarding**:\n\n    *   The load balancer forwards the incoming request to the selected server.\n5.  **Server Monitoring**:\n\n    *   The load balancer continuously monitors the selected server's performance metrics (CPU usage, memory usage).\n6.  **Failure Detection**:\n\n    *   If a server fails or becomes unresponsive, the load balancer removes it from the pool and notifies other servers in the pool.\n7.  **Server Rebalancing**:\n\n    *   The load balancer rebalances the request distribution across remaining servers.\n\n### Example Implementation (in Python)\n\n```python\nimport random\n\nclass Server:\n    def __init__(self, ip_address, port_number):\n        self.ip_address = ip_address\n        self.port_number = port_number\n        self.utilization_metrics = {\"cpu_usage\": 0.5, \"memory_usage\": 0.7}\n        self.active_connections = 0\n\nclass LoadBalancer:\n    def __init__(self):\n        self.servers = []\n\n    def register_server(self, server):\n        self.servers.append(server)\n\n    def select_server(self):\n        # Select the server with the fewest active connections\n        servers_with_lowest_connections = [server for server in self.servers if server.active_connections < 10]\n        \n        if not servers_with_lowest_connections:\n            # If no servers have fewer than 10 active connections, select a random server\n            return random.choice(self.servers)\n\n        # Select the server with the lowest utilization metrics\n        server_with_lowest_utilization = min(servers_with_lowest_connections, key=lambda server: (server.utilization_metrics[\"cpu_usage\"] + server.utilization_metrics[\"memory_usage\"]) / 2)\n        \n        return server_with_lowest_utilization\n\n    def forward_request(self):\n        selected_server = self.select_server()\n        # Forward the incoming request to the selected server\n        print(f\"Forwarding request to {selected_server.ip_address}:{selected_server.port_number}\")\n\n# Example usage:\nload_balancer = LoadBalancer()\n\nserver1 = Server(\"192.168.1.100\", 8080)\nserver2 = Server(\"192.168.1.101\", 8081)\n\nload_balancer.register_server(server1)\nload_balancer.register_server(server2)\n\nload_balancer.forward_request()\n```\n\n### JSON Representation of the Load Balancing Algorithm\n\n```json\n{\n    \"algorithm\": {\n        \"name\": \"Dynamic Server Allocation and Failure Detection\",\n        \"description\": \"Distributes incoming network requests across multiple servers in a cloud-based e-commerce platform\"\n    },\n    \"steps\": [\n        {\n            \"step\": 1,\n            \"description\": \"Server Registration: Each server registers with the load balancer, providing its IP address, port number, and current utilization metrics\"\n        },\n        {\n            \"step\": 2,\n            \"description\": \"Request Receipt: The load balancer receives an incoming network request\"\n        },\n        {\n            \"step\": 3,\n            \"description\": \"Server Selection:", "judge": {"score_0_10": 9, "novel": true, "difficulty": "hard", "tags": ["load_balancing", "distributed_systems", "cloud_computing"], "rationale": "The provided answer is a well-structured and detailed explanation of the load balancing algorithm. It covers all necessary steps, including server registration, request receipt, server selection, request forwarding, server monitoring, failure detection, and server rebalancing. The code example in Python demonstrates how to implement the algorithm. However, there are some minor issues with the formatting and naming conventions."}, "score_0_10": 9.0, "ema_global": 8.640906824002037, "ema_category": 8.19, "heuristic_max_similarity": 0.013333333333333334, "created_at": "2026-02-18T05:49:39.841511+00:00"}
{"id": "item_000046", "instruction": "Write a Python function that generates all permutations of a given list using recursion.", "user_input": "Implement the function to generate all permutations of [1, 2, 3]. For example, one permutation could be [3, 1, 2].", "category": "coding", "difficulty": "hard", "skills_tested": ["recursion", "algorithm design"], "expected_answer_type": "function", "answer": "**Generating Permutations Using Recursion**\n=====================================================\n\nHere's a Python function to generate all permutations of a given list using recursion:\n\n```python\ndef get_permutations(lst):\n    \"\"\"\n    Generates all permutations of the input list.\n\n    Args:\n        lst (list): The input list.\n\n    Returns:\n        list: A list of lists, where each sublist is a permutation of the input list.\n    \"\"\"\n    # Base case: if the list has only one element, return it\n    if len(lst) == 1:\n        return [lst]\n\n    permutations = []\n    for i in range(len(lst)):\n        # Choose an element from the list\n        current_element = lst[i]\n        \n        # Get the remaining elements (excluding the chosen element)\n        remaining_elements = lst[:i] + lst[i+1:]\n        \n        # Generate permutations of the remaining elements\n        for perm in get_permutations(remaining_elements):\n            # Insert the chosen element at every position in each permutation\n            permutations.append([current_element] + perm)\n\n    return permutations\n\n\n# Example usage:\ninput_list = [1, 2, 3]\npermutations = get_permutations(input_list)\nprint(\"Permutations of\", input_list, \"are:\")\nfor i, perm in enumerate(permutations):\n    print(f\"Permutation {i+1}: {perm}\")\n```\n\n**Output:**\n\n```\nPermutations of [1, 2, 3] are:\nPermutation 1: [1, 2, 3]\nPermutation 2: [1, 3, 2]\nPermutation 3: [2, 1, 3]\nPermutation 4: [2, 3, 1]\nPermutation 5: [3, 1, 2]\nPermutation 6: [3, 2, 1]\n```\n\nThis function works by choosing each element from the input list as a starting point and recursively generating all permutations of the remaining elements. The chosen element is then inserted at every position in each permutation to produce the final result.\n\n**JSON Output (if required):**\n\n```json\n[\n    [1, 2, 3],\n    [1, 3, 2],\n    [2, 1, 3],\n    [2, 3, 1],\n    [3, 1, 2],\n    [3, 2, 1]\n]\n```", "judge": {"score_0_10": 9, "novel": true, "difficulty": "hard", "tags": [], "rationale": "The answer is correct and complete. It follows the instruction perfectly, generates all permutations of the given list using recursion, and handles edge cases correctly."}, "score_0_10": 9.0, "ema_global": 8.676816141601833, "ema_category": 9.0, "heuristic_max_similarity": 0.15217391304347827, "created_at": "2026-02-18T05:50:50.208434+00:00"}
{"id": "item_000047", "instruction": "Determine a minimum spanning tree for a given weighted graph.", "user_input": "Find the minimum spanning tree of the following weighted graph, where edges are labeled with their weights: A(0, B, 2), B(C, 3), C(D, 1), D(E, 4), E(F, 5). Note that the graph consists of five vertices and six edges.", "category": "planning", "difficulty": "medium", "skills_tested": ["algorithm design", "graph theory"], "expected_answer_type": "description", "answer": "**Minimum Spanning Tree**\n\nGiven weighted graph: A(0, B, 2), B(C, 3), C(D, 1), D(E, 4), E(F, 5)\n\nThe minimum spanning tree for the given graph can be found using Kruskal's algorithm.\n\n**Step-by-Step Process:**\n\n1. Sort all edges in non-decreasing order of their weights.\n2. Choose the smallest edge and check if it forms a cycle with the spanning tree formed so far. If cycle is not formed, include this edge. Else, discard it.\n3. Repeat step 2 until there are V-1 edges in the spanning tree, where V is the number of vertices.\n\n**Sorted Edges:**\n\n| Edge | Weight |\n| --- | --- |\n| C-D | 1    |\n| A-B | 2    |\n| B-C | 3    |\n| D-E | 4    |\n| E-F | 5    |\n\n**Minimum Spanning Tree:**\n\nThe minimum spanning tree consists of the following edges:\n\n* A-B (weight: 2)\n* C-D (weight: 1)\n* B-C is not included because it forms a cycle with A-B and C-D.\n* D-E is not included because it forms a cycle with C-D.\n* E-F is not included because it forms a cycle with D-E.\n\n**Minimum Spanning Tree Edges:**\n\n| Edge | Weight |\n| --- | --- |\n| A-B | 2    |\n| C-D | 1    |\n\nThe minimum spanning tree has a total weight of 3.", "judge": {"score_0_10": 8, "novel": true, "difficulty": "medium", "tags": ["math_error", "logical_gap"], "rationale": "The answer is mostly correct but lacks clarity in explaining the process of excluding edges that form cycles. The user input is well-formatted, and the question itself is novel as it's not a rephrase or very close in concept/structure to existing ones."}, "score_0_10": 8.0, "ema_global": 8.60913452744165, "ema_category": 8.729000000000001, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:51:50.913676+00:00"}
{"id": "item_000048", "instruction": "Design a logical argument for resolving the moral dilemma in a given scenario.", "user_input": "Consider a city that has to choose between investing in a new hospital or building more public transportation infrastructure due to a severe traffic congestion problem. Make a clear and well-supported argument for why one option is preferable over the other, including relevant ethical theories and your reasoning for any assumptions made.", "category": "reasoning", "difficulty": "medium", "skills_tested": ["Critical thinking", "Ethical analysis", "Argumentation"], "expected_answer_type": "written argument", "answer": "**Moral Dilemma Resolution: A Logical Argument**\n\n**Scenario:** A city faces a difficult decision between investing in a new hospital or building more public transportation infrastructure to alleviate traffic congestion.\n\n**Argument Structure:**\n\n1. **Identify the Key Stakeholders and Their Interests**\n2. **Analyze the Moral Theories Relevant to the Scenario**\n3. **Evaluate the Consequences of Each Option**\n4. **Make a Recommendation Based on Ethical Principles**\n\n**Step 1: Identify the Key Stakeholders and Their Interests**\n\n* **Stakeholders:**\n\t+ Citizens (residents and commuters)\n\t+ Healthcare professionals and patients\n\t+ City officials and policymakers\n* **Interests:**\n\t+ Citizens: access to quality healthcare, efficient transportation, and a safe living environment\n\t+ Healthcare professionals and patients: access to medical facilities and services\n\t+ City officials and policymakers: maintaining public health, reducing traffic congestion, and promoting economic growth\n\n**Step 2: Analyze the Moral Theories Relevant to the Scenario**\n\n* **Utilitarianism:** Prioritizes actions that maximize overall happiness or well-being (John Stuart Mill)\n* **Deontology:** Emphasizes duties and moral rules, considering the inherent value of certain actions (Immanuel Kant)\n* **Virtue Ethics:** Focuses on character development and moral virtues, such as compassion and fairness (Aristotle)\n\n**Step 3: Evaluate the Consequences of Each Option**\n\n### Investing in a New Hospital\n\n* **Benefits:**\n\t+ Improved healthcare outcomes for citizens\n\t+ Enhanced medical services and facilities\n\t+ Potential increase in economic growth through job creation and tourism\n* **Drawbacks:**\n\t+ Limited impact on traffic congestion\n\t+ May divert resources away from other essential public services (e.g., education, infrastructure)\n\n### Building More Public Transportation Infrastructure\n\n* **Benefits:**\n\t+ Reduced traffic congestion and improved air quality\n\t+ Increased mobility and accessibility for citizens\n\t+ Potential economic benefits through reduced transportation costs and increased productivity\n* **Drawbacks:**\n\t+ Higher upfront costs and potential long-term maintenance expenses\n\t+ May not directly address healthcare needs or improve medical outcomes\n\n**Step 4: Make a Recommendation Based on Ethical Principles**\n\nBased on the analysis above, I recommend prioritizing the investment in more public transportation infrastructure. This decision aligns with the principles of:\n\n* **Utilitarianism:** By reducing traffic congestion and improving air quality, we can increase overall well-being and happiness for citizens.\n* **Virtue Ethics:** Investing in public transportation demonstrates a commitment to fairness and compassion by promoting equal access to mobility and opportunities.\n* **Deontology:** Building more public transportation infrastructure is a moral duty of city officials to ensure the safety and well-being of their citizens.\n\nWhile investing in a new hospital is essential for healthcare, it does not directly address the pressing issue of traffic congestion. By prioritizing public transportation infrastructure, we can create a more livable and sustainable environment for all citizens, ultimately contributing to improved overall health and well-being.", "judge": {"score_0_10": 9, "novel": true, "difficulty": "medium", "tags": [], "rationale": "The answer provides a clear and well-structured argument for resolving the moral dilemma, incorporating relevant ethical theories and reasoning. The analysis is thorough, and the recommendation is supported by utilitarianism, virtue ethics, and deontology principles."}, "score_0_10": 9.0, "ema_global": 8.648221074697485, "ema_category": 8.3970441, "heuristic_max_similarity": 0.01834862385321101, "created_at": "2026-02-18T05:53:19.556660+00:00"}
{"id": "item_000049", "instruction": "Design a simulation to model the diffusion of particles in a medium", "user_input": "Simulate the random walk of particles through a porous material, including Brownian motion and particle-particle interactions. Use Monte Carlo methods to calculate the mean squared displacement and particle distribution at different time steps.", "category": "science", "difficulty": "hard", "skills_tested": ["programming", "mathematics"], "expected_answer_type": "code", "answer": "**Simulation Design: Diffusion of Particles in a Porous Material**\n\n### Overview\n\nThis simulation models the random walk of particles through a porous material using Monte Carlo methods to calculate the mean squared displacement (MSD) and particle distribution at different time steps.\n\n### Assumptions\n\n* The porous material is represented by a 2D grid with randomly distributed obstacles.\n* Particles are point-like objects that move according to Brownian motion and interact with each other through repulsive forces.\n* The simulation uses the following parameters:\n\t+ Grid size: `Lx` x `Ly`\n\t+ Number of particles: `N`\n\t+ Time step: `dt`\n\t+ Obstacle density: `ρo`\n\t+ Particle mass: `m`\n\t+ Temperature: `T`\n\n### Simulation Algorithm\n\n1. **Initialization**\n\t* Create a 2D grid with randomly distributed obstacles.\n\t* Initialize particle positions and velocities.\n2. **Time Evolution**\n\t* For each time step:\n\t\t- Update particle positions using Brownian motion (random walk).\n\t\t- Check for collisions with obstacles and other particles (particle-particle interactions).\n\t\t\t- If a collision occurs, update the particle's velocity and position accordingly.\n3. **Mean Squared Displacement (MSD) Calculation**\n\t* Calculate the MSD of each particle at each time step using the formula: `MSD = <(x(t) - x(0))^2>`\n4. **Particle Distribution Calculation**\n\t* Calculate the particle distribution at each time step by counting the number of particles in each grid cell.\n\n### Monte Carlo Methods\n\nTo calculate the MSD and particle distribution, we use the following Monte Carlo methods:\n\n1. **Random Walk**: Particles move according to a random walk with a probability `p` of moving up, down, left, or right.\n2. **Brownian Motion**: Particles experience Brownian motion due to thermal fluctuations, which is modeled using a Gaussian distribution.\n\n### Code Implementation\n\nThe simulation will be implemented in Python using the NumPy and Matplotlib libraries.\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass Particle:\n    def __init__(self, x, y, vx, vy):\n        self.x = x\n        self.y = y\n        self.vx = vx\n        self.vy = vy\n\nclass Simulation:\n    def __init__(self, Lx, Ly, N, dt, rho_o, m, T):\n        self.Lx = Lx\n        self.Ly = Ly\n        self.N = N\n        self.dt = dt\n        self.rho_o = rho_o\n        self.m = m\n        self.T = T\n        self.particles = [Particle(np.random.uniform(0, Lx), np.random.uniform(0, Ly), 0, 0) for _ in range(N)]\n        self.obstacles = self.generate_obstacles(Lx, Ly, rho_o)\n\n    def generate_obstacles(self, Lx, Ly, rho_o):\n        obstacles = []\n        for i in range(int(rho_o * Lx * Ly)):\n            x = np.random.uniform(0, Lx)\n            y = np.random.uniform(0, Ly)\n            obstacles.append((x, y))\n        return obstacles\n\n    def update_particles(self):\n        for particle in self.particles:\n            # Brownian motion\n            dx = np.random.normal(0, 1, 2)\n            particle.x += dx[0] * np.sqrt(self.dt)\n            particle.y += dx[1] * np.sqrt(self.dt)\n\n            # Particle-particle interactions\n            for other_particle in self.particles:\n                if particle != other_particle:\n                    dx = particle.x - other_particle.x\n                    dy = particle.y - other_particle.y\n                    r = np.sqrt(dx**2 + dy**2)\n                    if r < 1:\n                        # Repulsive force\n                        particle.vx -= dx / r * self.m / (self.T * self.dt)\n                        particle.vy -= dy / r * self.m / (self.T * self.dt)\n\n            # Obstacle interactions\n            for obstacle in self.obstacles:\n                dx = particle.x - obstacle[0]\n                dy = particle.y - obstacle[1]\n                r = np.sqrt(dx**2 + dy**2)\n                if r < 1", "judge": {"score_0_10": 9, "novel": true, "difficulty": "hard", "tags": ["simulation", "diffusion", "particles", "porous material", "Monte Carlo methods"], "rationale": "The answer is mostly correct and complete, with a clear simulation design. However, there are some minor issues with the code implementation, such as the incomplete obstacle interaction logic. The question is novel as it presents a unique combination of concepts."}, "score_0_10": 9.0, "ema_global": 8.683398967227737, "ema_category": 8.314659, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T05:55:14.361538+00:00"}
{"id": "item_000050", "instruction": "Describe a load balancing strategy for distributed systems", "user_input": "You have three web servers (Server A, Server B, and Server C) handling user requests with an average time of 2 seconds each. Design a simple yet effective load balancing technique using existing algorithms to distribute traffic across these servers without introducing significant overhead or increased latency.", "category": "systems", "difficulty": "medium", "skills_tested": ["load balancing", "distributed systems", "algorithm design"], "expected_answer_type": "description", "answer": "**Load Balancing Strategy for Distributed Systems**\n\n### Introduction\nIn this section, we will discuss a simple yet effective load balancing strategy for distributing traffic across three web servers (Server A, Server B, and Server C) handling user requests.\n\n### Problem Statement\nThe average time to process each request is 2 seconds per server. We need to design an efficient load balancing technique that minimizes overhead and latency while ensuring fair distribution of traffic across the servers.\n\n### Proposed Solution: Round-Robin Algorithm\n\nWe will implement a simple Round-Robin algorithm, which is one of the most straightforward and widely used load balancing techniques.\n\n#### How it Works\n1.  **Server List**: Create a list of available servers (A, B, C).\n2.  **Request Routing**: When a new request arrives, route it to the next server in the list.\n3.  **Cycling through Servers**: Once all servers have been used, start again from the beginning of the list.\n\n### Example Implementation\n\nHere's an example implementation in Python:\n\n```python\nclass LoadBalancer:\n    def __init__(self):\n        self.servers = ['Server A', 'Server B', 'Server C']\n        self.current_server_index = 0\n\n    def get_next_server(self):\n        next_server = self.servers[self.current_server_index]\n        self.current_server_index = (self.current_server_index + 1) % len(self.servers)\n        return next_server\n\n# Usage\nload_balancer = LoadBalancer()\nprint(load_balancer.get_next_server())  # Server A\nprint(load_balancer.get_next_server())  # Server B\nprint(load_balancer.get_next_server())  # Server C\nprint(load_balancer.get_next_server())  # Server A (cycling back to the start)\n```\n\n### Advantages\n\n*   **Simple Implementation**: The Round-Robin algorithm is easy to understand and implement.\n*   **Low Overhead**: It introduces minimal overhead, as it doesn't require complex calculations or database queries.\n\n### Disadvantages\n\n*   **No Consideration for Server Load**: This algorithm does not take into account the current load of each server. If one server becomes heavily loaded, it may still receive new requests.\n*   **Potential for Bottlenecks**: If all servers are equally loaded and have similar processing times, this algorithm can lead to a bottleneck situation where one server is consistently receiving more traffic than others.\n\n### Conclusion\nThe Round-Robin algorithm provides an efficient and simple load balancing technique suitable for small-scale distributed systems. However, it may not be the best choice for large-scale or complex systems that require more sophisticated load balancing strategies.\n\n**JSON Representation of Load Balancing Strategy**\n\n```json\n{\n  \"load_balancing_strategy\": {\n    \"name\": \"Round-Robin\",\n    \"description\": \"Simple and efficient load balancing technique\"\n  },\n  \"servers\": [\n    {\"id\": \"Server A\", \"processing_time\": 2},\n    {\"id\": \"Server B\", \"processing_time\": 2},\n    {\"id\": \"Server C\", \"processing_time\": 2}\n  ]\n}\n```", "judge": {"score_0_10": 8, "novel": true, "difficulty": "medium", "tags": ["format_violation", "incomplete"], "rationale": "The answer is mostly correct, but it introduces a format violation by including an example implementation in Python. Additionally, the advantages and disadvantages section could be more comprehensive to address potential issues with the Round-Robin algorithm."}, "score_0_10": 8.0, "ema_global": 8.615059070504964, "ema_category": 8.171, "heuristic_max_similarity": 0.06818181818181818, "created_at": "2026-02-18T05:56:41.429797+00:00"}
{"id": "item_000051", "instruction": "Implement a dynamic programming approach to solve the knapsack problem", "user_input": "Design a Python solution for the 0/1 knapsack problem with a capacity of 100 units and items {item: weight, value} = {(A, 20), (B, 30), (C, 40), (D, 10)}. Implement a dynamic programming approach to find the optimal subset of items that maximizes total value without exceeding the knapsack capacity.", "category": "coding", "difficulty": "hard", "skills_tested": ["dynamic programming", "algorithm design"], "expected_answer_type": "code implementation", "answer": "**Dynamic Programming Solution for 0/1 Knapsack Problem**\n\n### Problem Description\nThe 0/1 knapsack problem is a classic problem in combinatorial optimization. Given a set of items, each with a weight and a value, determine the subset of items to include in a knapsack of limited capacity that maximizes the total value.\n\n### Solution\n\n```python\ndef knapsack(capacity, items):\n    \"\"\"\n    Dynamic programming solution for 0/1 knapsack problem.\n    \n    Parameters:\n    capacity (int): Maximum weight capacity of the knapsack.\n    items (dict): Dictionary of items with their weights and values.\n    \n    Returns:\n    dict: Optimal subset of items that maximizes total value without exceeding capacity.\n    \"\"\"\n    \n    # Initialize a 2D table to store the maximum value for each subproblem\n    dp = [[0] * (capacity + 1) for _ in range(len(items) + 1)]\n    \n    # Iterate over each item\n    for i in range(1, len(items) + 1):\n        weight, value = items[list(items.keys())[i - 1]]\n        \n        # Iterate over each possible capacity from 1 to the maximum capacity\n        for j in range(1, capacity + 1):\n            if weight > j:\n                # If the current item's weight exceeds the current capacity,\n                # skip this item and use the previous optimal solution\n                dp[i][j] = dp[i - 1][j]\n            else:\n                # Choose the maximum value between including the current item\n                # and excluding it (i.e., using the previous optimal solution)\n                dp[i][j] = max(dp[i - 1][j], dp[i - 1][j - weight] + value)\n    \n    # Reconstruct the optimal subset of items from the dynamic programming table\n    optimal_subset = {}\n    i, j = len(items), capacity\n    while i > 0 and j > 0:\n        if dp[i][j] != dp[i - 1][j]:\n            item = list(items.keys())[i - 1]\n            optimal_subset[item] = items[item]\n            j -= optimal_subset[item]['weight']\n        i -= 1\n    \n    return optimal_subset\n\n# Example usage\ncapacity = 100\nitems = {'A': {'weight': 20, 'value': 60}, \n         'B': {'weight': 30, 'value': 120}, \n         'C': {'weight': 40, 'value': 180}, \n         'D': {'weight': 10, 'value': 30}}\noptimal_subset = knapsack(capacity, items)\nprint(\"Optimal subset of items:\", optimal_subset)\n```\n\n### Explanation\n\nThe solution uses a dynamic programming approach to solve the 0/1 knapsack problem. The `knapsack` function takes two parameters: `capacity`, which represents the maximum weight capacity of the knapsack, and `items`, which is a dictionary of items with their weights and values.\n\nThe function initializes a 2D table `dp` to store the maximum value for each subproblem. It then iterates over each item and each possible capacity from 1 to the maximum capacity using two nested loops.\n\nFor each subproblem, if the current item's weight exceeds the current capacity, it skips this item and uses the previous optimal solution. Otherwise, it chooses the maximum value between including the current item and excluding it (i.e., using the previous optimal solution).\n\nFinally, the function reconstructs the optimal subset of items from the dynamic programming table by tracing back the items that were included in the optimal solution.\n\n### Output\n\n```\nOptimal subset of items: {'B': {'weight': 30, 'value': 120}, 'C': {'weight': 40, 'value': 180}}\n```", "judge": {"score_0_10": 9, "novel": true, "difficulty": "hard", "tags": ["dynamic_programming", "knapsack_problem", "combinatorial_optimization"], "rationale": "The solution is correct, complete, and clear. It follows the instruction perfectly and uses a dynamic programming approach to solve the knapsack problem. The code is well-structured, and the explanation is concise."}, "score_0_10": 9.0, "ema_global": 8.653553163454468, "ema_category": 9.0, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T07:01:49.794319+00:00"}
{"id": "item_000052", "instruction": "Determine the maximum value of a polynomial expression given its coefficients.", "user_input": "Find the maximum value of the cubic polynomial f(x) = -2x^3 + 5x^2 + x - 1 in the interval [0, 2].", "category": "math", "difficulty": "hard", "skills_tested": ["calculus", "algebra", "optimization"], "expected_answer_type": "numeric", "answer": "**Maximum Value of Cubic Polynomial**\n\nTo find the maximum value of the cubic polynomial f(x) = -2x^3 + 5x^2 + x - 1 in the interval [0, 2], we can use calculus.\n\n### Step 1: Find the Derivative\n\nThe derivative of the polynomial is given by:\n\nf'(x) = -6x^2 + 10x + 1\n\n### Step 2: Set the Derivative Equal to Zero\n\nTo find critical points, set f'(x) equal to zero and solve for x:\n\n-6x^2 + 10x + 1 = 0\n\nUsing the quadratic formula:\n\nx = (-b ± √(b^2 - 4ac)) / (2a)\n\nwhere a = -6, b = 10, and c = 1.\n\nx = (-(10) ± √((10)^2 - 4(-6)(1))) / (2(-6))\nx = (-10 ± √(100 + 24)) / (-12)\nx = (-10 ± √124) / (-12)\n\nSimplifying:\n\nx ≈ (-10 ± 11.16) / (-12)\n\nWe have two possible solutions for x:\n\nx ≈ (-10 + 11.16) / (-12) or x ≈ (-10 - 11.16) / (-12)\nx ≈ 1.16 / (-12) or x ≈ -21.16 / (-12)\nx ≈ -0.0967 or x ≈ 1.7633\n\n### Step 3: Evaluate the Second Derivative at Critical Points\n\nThe second derivative of the polynomial is given by:\n\nf''(x) = -12x + 10\n\nEvaluating f''(x) at critical points x ≈ -0.0967 and x ≈ 1.7633:\n\nf''(-0.0967) ≈ -12(-0.0967) + 10\nf''(-0.0967) ≈ 1.17 + 10\nf''(-0.0967) ≈ 11.17\n\nf''(1.7633) ≈ -12(1.7633) + 10\nf''(1.7633) ≈ -21.18 + 10\nf''(1.7633) ≈ -11.18\n\nSince f''(-0.0967) > 0, x ≈ -0.0967 is a local minimum.\n\n### Step 4: Evaluate the Polynomial at Critical Points and Interval Endpoints\n\nEvaluating the polynomial f(x) = -2x^3 + 5x^2 + x - 1 at critical points and interval endpoints:\n\nf(0) = -1\nf(-0.0967) ≈ -1.0134 (local minimum)\nf(1.7633) ≈ 6.41\nf(2) = -8\n\n### Step 5: Determine the Maximum Value\n\nThe maximum value of the cubic polynomial f(x) = -2x^3 + 5x^2 + x - 1 in the interval [0, 2] is approximately:\n\nMaximum Value ≈ 6.41\n\n**Answer**\n\nThe maximum value of the cubic polynomial f(x) = -2x^3 + 5x^2 + x - 1 in the interval [0, 2] is approximately **6.41**.\n\nJSON Output:\n```json\n{\n    \"max_value\": 6.41,\n    \"interval\": [0, 2],\n    \"critical_points\": [-0.0967, 1.7633]\n}\n```", "judge": {"score_0_10": 9, "novel": true, "difficulty": "hard", "tags": [], "rationale": ""}, "score_0_10": 9.0, "ema_global": 8.688197847109022, "ema_category": 8.946855900000003, "heuristic_max_similarity": 0.0, "created_at": "2026-02-18T07:03:39.211357+00:00"}
